{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HOTOSM's Docs","text":"<p>\ud83d\udcd6 Welcome to the HOT Technical documentation for all of our open-source tools.</p> <p>This website is primary an index linking to the documentation for each specific tool, including including setup and usage instructions, API documentation, and product roadmaps.</p> <p>This documentation is available under the <code>Projects</code> section in the sidebar.</p> <p>The following additional documentation is provided:</p> <ul> <li>A high level overview of how our tools inter-relate and can be used to   form an End-To-End mapping workflow.</li> <li>Information on our development practices at HOT.</li> <li>Our community code of conduct for contribution.</li> <li>Our privacy policy for most of our tools.</li> <li>Developer guidelines and information to assist the development workflow.</li> </ul> <p>HOT is working towards the modularization of our code to improve maintainability and implement an end-to-end user flow between multiple tools. There is more info about modularization of our code.</p>"},{"location":"#the-hot-ecosystem-of-tools","title":"The HOT Ecosystem of Tools","text":""},{"location":"LICENSE/","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions","text":"<p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified   it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is   released under this License and any conditions added under   section 7. This requirement modifies the requirement in section 4   to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this   License to anyone who comes into possession of a copy. This   License will therefore apply, along with any applicable section 7   additional terms, to the whole of the work, and all its parts,   regardless of how they are packaged. This License gives no   permission to license the work in any other way, but it does not   invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display   Appropriate Legal Notices; however, if the Program has interactive   interfaces that do not display Appropriate Legal Notices, your   work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product   (including a physical distribution medium), accompanied by the   Corresponding Source fixed on a durable physical medium   customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product   (including a physical distribution medium), accompanied by a   written offer, valid for at least three years and valid for as   long as you offer spare parts or customer support for that product   model, to give anyone who possesses the object code either (1) a   copy of the Corresponding Source for all the software in the   product that is covered by this License, on a durable physical   medium customarily used for software interchange, for a price no   more than your reasonable cost of physically performing this   conveying of source, or (2) access to copy the Corresponding   Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the   written offer to provide the Corresponding Source. This   alternative is allowed only occasionally and noncommercially, and   only if you received the object code with such an offer, in accord   with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated   place (gratis or for a charge), and offer equivalent access to the   Corresponding Source in the same way through the same place at no   further charge. You need not require recipients to copy the   Corresponding Source along with the object code. If the place to   copy the object code is a network server, the Corresponding Source   may be on a different server (operated by you or a third party)   that supports equivalent copying facilities, provided you maintain   clear directions next to the object code saying where to find the   Corresponding Source. Regardless of what server hosts the   Corresponding Source, you remain obligated to ensure that it is   available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,   provided you inform other peers where the object code and   Corresponding Source of the work are being offered to the general   public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the   terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or   author attributions in that material or in the Appropriate Legal   Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,   or requiring that modified versions of such material be marked in   reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors   or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some   trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that   material by anyone who conveys the material (or modified versions   of it) with contractual assumptions of liability to the recipient,   for any liability that these contractual assumptions directly   impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"become-a-contributor/","title":"\ud83c\udf89 Become a Contributor \ud83c\udf89","text":""},{"location":"become-a-contributor/#background","title":"Background","text":"<p>HOT has a small tech team of motivated individuals that work to:</p> <ul> <li>Develop many different tools you can see in our Github organization</li> <li>Improve accessibility of humanitarian data to the wider community.</li> <li>Support field activities for data collection and analysis.</li> </ul> <p>Being quite stretched, we rely heavily on the support of volunteers for all of the above activities!</p>"},{"location":"become-a-contributor/#getting-started","title":"Getting Started","text":"<p>We welcome and encourage contributors of all skill levels, and we are committed to making sure your participation is inclusive, enjoyable, and rewarding.</p> <p>If you have never contributed to an open source project before, we are a good place to start, and we will make sure you are supported every step of the way.</p> <p>If you have any questions, please ask our community \ud83d\ude0a</p>"},{"location":"become-a-contributor/#what-can-i-help-with","title":"What Can I Help With?","text":""},{"location":"become-a-contributor/#testing","title":"\ud83e\uddea Testing","text":"<p>We warmly welcome your input in testing our tools and sharing your feedback.</p> <p>This could be via either:</p> <ul> <li>User testing: please use our tools and inform us of any errors or issues you   may encounter.</li> </ul> <ul> <li>Technical testing: if you are a software tester, then improvements to our automated   testing suite would be greatly appreciated!</li> </ul> <p>Feedback can be given either via the Github repository for the tool, or via our community discussion channels.</p>"},{"location":"become-a-contributor/#code","title":"\ud83d\udc68\u200d\ud83d\udcbb Code","text":"<p>Contributing code is a certain way to benefit the team and the end-user communities of our tools!</p> <p>Please browse our latest task boards, or Github issues, found on the <code>Projects</code> section on this website.</p> <p>A pull request (PR) can be used to solve existing issues on a repository \ud83d\ude4c</p> <p>If you are wondering where to start, you can filter by the good first issue label.</p> <p>Skills with the following would be beneficial:</p> <ul> <li>Python</li> <li>FastAPI</li> <li>TypeScript / JavaScript</li> <li>React</li> <li>Docker</li> <li>CI/CD workflows</li> </ul>"},{"location":"become-a-contributor/#report-bugs-suggest-improvements","title":"\ud83d\udcdd Report bugs / suggest improvements","text":"<p>The issue queue for a tool on Github is the best way to get started here.</p> <p>There are issue templates for BUGs and FEATURES that you can use, or you could also create your own.</p>"},{"location":"become-a-contributor/#discussions","title":"\ud83d\udcac Discussions \ud83d\udcac","text":"<p>Your input into the direction of the tool, features to include, or how they should be implemented are valuable.</p> <p>Please use Github discussions or the community discussion channels to discuss topics and provide your insight.</p>"},{"location":"become-a-contributor/#report-security-vulnerabilities","title":"\ud83d\udd12 Report security vulnerabilities","text":"<p>Please inform a maintainer as soon as possible, including the CVE code.</p> <p>Message via the HOTOSM Slack or direct email would be preferred, but via Github issue is also possible.</p>"},{"location":"become-a-contributor/#how-to-get-started","title":"\ud83c\udfc3 How to get started","text":"<p>We have had a few people ask how they can get started on open-source projects.</p> <p>A summarized brief workflow is:</p> <ol> <li>See an issue you want to work on.</li> <li>Comment on the issue to seek any clarifications if needed (the maintainer    will provide further details as needed).</li> <li>(Optional) The maintainer can assign the issue to you if you like (for    tracking / to indicate to others you are working on it).</li> <li>Fork the repo.</li> <li>Clone your fork to your machine and make a branch.</li> <li>Work on your branch.</li> <li>Test your changes locally.</li> <li>Push the fork to your repo.</li> <li>Go to Github and create a Pull Request against the original repo.</li> <li>Provide a description of what you did, what issue it related to, how to     test it, and other relevant info (often there will be a template to follow).</li> <li>The maintainer reviews &amp; comments where needed.</li> <li>(Optional) If changes / fixes are needed, then are made and push to the branch     (the PR gets updated automatically).</li> <li>Changes get merged when ready &amp; you are now officially a contributor!</li> </ol>"},{"location":"become-a-contributor/#thank-you","title":"\ud83e\udd1d Thank You","text":"<p>Thank you very much in advance for your contributions!!</p> <p>Please ensure you refer to our Code of Conduct.</p> <p>If you've read the guidelines, but are still not sure how to contribute on Github, please reach out to us via our Slack #geospatial-tech-and-innovation channel.</p>"},{"location":"code-of-conduct/","title":"\ud83d\udcdc Code of conduct","text":"<p>(The latest version can be found at https://www.hotosm.org/code-of-conduct</p> <p>Welcome to Humanitarian OpenStreetMap Team. HOT is committed to providing a welcoming and safe environment for people of all races, gender identities, gender expressions, sexual orientations, physical abilities, physical appearances, socio-economic backgrounds, nationalities, ages, religions, and beliefs.</p> <p>The HOT community principles are:</p> <ul> <li>Be friendly and patient. Be generous and kind in both giving and accepting   critique. Critique is a natural and important part of our culture. Good   critiques are kind, respectful, clear, and constructive, focused on goals and   requirements rather than personal preferences. You are expected to give and   receive criticism with grace. Be considerate in speech and actions, and   actively seek to acknowledge and respect the boundaries of fellow attendees.</li> </ul> <ul> <li> <p>Be welcoming. We strive to be a community that welcomes and supports   people of all backgrounds and identities. Some examples of behavior that   contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language.</li> </ul> <ul> <li>Being respectful of differing viewpoints and experiences.</li> </ul> <ul> <li>Gracefully accepting constructive criticism.</li> </ul> <ul> <li>Showing empathy towards other community members.</li> </ul> <ul> <li>Placing collective interest before your own interest.</li> </ul> </li> </ul> <ul> <li>Be considerate. Your work will be used by other people, and you in turn   will depend on the work of others. Any decision you take will affect users and   colleagues, and you should take those consequences into account when making   decisions. Remember that we're a world-wide community, so you might not be   communicating in someone else's primary language.</li> </ul> <ul> <li>Be respectful. Not all of us will agree all the time, but disagreement is   no excuse for poor behavior and poor manners. We might all experience some   frustration now and then, but we cannot allow that frustration to turn into a   personal attack. It's important to remember that a community where people feel   uncomfortable or threatened is not a productive one. Members of the HOT   community should be respectful when dealing with other members as well as with   people outside the HOT community.</li> </ul> <ul> <li> <p>Be careful in your word choice. We are a global community of   professionals, and we conduct ourselves professionally. Be kind to others. Do   not insult or put down other participants. Harassment and other exclusionary   behavior aren't acceptable. This includes, but is not limited to:</p> <ul> <li>Violent threats or language directed against another person.</li> </ul> <ul> <li>Discriminatory jokes and language.</li> </ul> <ul> <li>Posting sexually explicit or violent material.</li> </ul> <ul> <li>Posting (or threatening to post) other people's personally identifying   information (\"doxing\").</li> </ul> <ul> <li>Personal insults, especially those using racist or sexist terms.</li> </ul> <ul> <li>Unwelcome sexual attention.</li> </ul> <ul> <li>Advocating for, or encouraging, any of the above behavior.</li> </ul> <ul> <li>Repeated harassment of others. In general, if someone asks you to stop, then   stop.</li> </ul> </li> </ul> <ul> <li>Assume all communications are positive. Always remain polite, and assume   good faith. It is surprisingly easy to misunderstand each other, be it online   or in person, particularly in such a culturally diverse setting as ours.   Misunderstandings are particularly easy to arise when we are in a rush, or   otherwise distracted. Please ask clarifying questions before assuming that a   communication was inappropriate.</li> </ul> <ul> <li>When we disagree, try to understand why. Disagreements, both social and   technical, happen easily and often. It is important that we resolve such   disagreements and differing views constructively. At times it can be hard to   appreciate a viewpoint that contradicts your own perceptions. Instead of pushing   back, try to understand where the other person is coming from, and don't be   afraid to ask questions. You can be most helpful if your own replies serve to   clarify, rather than to escalate an issue. Also don't forget that it can be   easy to make mistakes, and allow for the possibility that the mistake may have   been yours. When this happens it is better to resolve the issue together, and   to learn from the experience together, than to place blame.</li> </ul> <p>Original text courtesy of the Speak Up! project.</p> <p>Further sources:</p> <ul> <li>Ada Initiative: HOWTO design a code of conduct for your community</li> </ul> <ul> <li>Algorithm Club Code of Conduct</li> </ul> <ul> <li>American Red Cross GIS Team Code of Conduct</li> </ul> <ul> <li>Contributor Covenant \u2013 A Code of Conduct for Open Source Projects</li> </ul> <ul> <li>Django Code of Conduct</li> </ul> <ul> <li>Mozilla Community Participation Guidelines</li> </ul> <ul> <li>Vox Media Code of Conduct</li> </ul>"},{"location":"code-of-conduct/#complaint-handling-process","title":"Complaint Handling Process","text":"<p>As a first measure, it is preferable to work out issues directly with the people involved, or to work with other Community Members who can help you resolve the issue. This may take several forms:</p> <ul> <li>Talk with one another. Assume that communications are positive and that people   are treating each other with respect. Cues about emotions are often lacking   from digital communications. Many of our modes of digital communication tend   towards brevity, which can be easier to interpret incorrectly as being negative.</li> </ul> <ul> <li>Contact a representative of the Community Working Group, which exists to   support the HOT Community. Representatives are available to discuss any   concerns about behaviour within the community, or ideas to promote positive   behaviours. You can email them at   community@hotosm.org.</li> </ul> <ul> <li>Contact a representative of the Governance Working Group, which drafted   these recommendations and the CoC. Representatives are available to provide   advice on particular scenarios, as well as on the processes around the CoC.</li> </ul> <ul> <li>Contact the HOT Chair of Voting Members.</li> </ul> <ul> <li>Contact a HOT Board Member. Board members are well versed in the   community and its management. They can offer advice on your particular   situation, and know the resources of the organization that may be available to   you.</li> </ul> <ul> <li>Contact the HOT Community Partnerships Manager.</li> </ul> <p>When these informal processes fail, or when a situation warrants an immediate response by HOT, you can evoke the HOT Policy and Code of Conduct Complaint Handling Process. This process was adopted by HOT Voting Members in 2016 to provide a more formal means of enforcement for our community standards. You start it by emailing complaints@hotosm.org with a description of your complaint, your name, and the name of the offending party. All complaints will be considered confidential. The full process is described here .</p>"},{"location":"dev-practices/","title":"HOT's Development Practices","text":"<p>This was last updated on 12/07/2024 and details our idealised software development practices.</p>"},{"location":"dev-practices/#open-source-model","title":"Open-Source Model","text":"<ul> <li>All code developed at HOT is open-source.<ul> <li>We rely heavily on community feedback to steer our development.</li> <li>Contribution from anyone is welcome.</li> </ul> </li> <li>We follow a git-flow, or fork-and-pr model:<ul> <li>Developers should create a fork of the repo they wish to modify.</li> <li>Code is added to a branch inside the fork.</li> <li>A pull request (merge request or PR) is made against the development   branch of the original repository.</li> </ul> </li> <li>Our code is generally licensed   AGPLv3   unless otherwise specified.</li> </ul> <p>Note</p> <p>Some tools still use BSD-2-Clause: - Tasking Manager - Export Tool - Open Aerial Map</p> <p>More details here</p>"},{"location":"dev-practices/#defining-what-we-build","title":"Defining What We Build","text":""},{"location":"dev-practices/#gathering-requirements","title":"Gathering Requirements","text":"<ul> <li>When a project is first conceived, we have an idea of requirements / direction.</li> <li>Over time the requirements are dynamic and interative. We have two approaches.</li> </ul>"},{"location":"dev-practices/#translating-user-requirements-into-actions","title":"Translating User Requirements Into Actions","text":"<ul> <li>Our main feedback mechanism is through user testing. This should be done in   collaboration with our hubs and partners to determine: what works well, and what   doesn't. The general public will also use our tools and provide feedback.</li> <li>For field-based tools (FMTM, DroneTM), this would involve running mapping campaigns   and gathering feedback from the teams involved.</li> <li>For desk-based tools (everything else), we mostly gather feedback through informal   channels such as Slack.</li> <li>All tools have a public channel on Slack for discussion, providing feedback, and   gathering requirements.</li> <li> <p>Once we have gathered feedback, this should be divided up into actionable Git   issues by the PO / Tech Lead and grouped into milestones or releases.</p> <p>See the milestones and releases sections for details about grouping tasks into themes.</p> </li> </ul>"},{"location":"dev-practices/#gathering-direct-technical-feedback","title":"Gathering Direct Technical Feedback","text":"<ul> <li>Users that are more tech-focused can create issues directly on our Git project.</li> <li>Discussions can also be used on our Git tracker for more general user input or   feedback.</li> <li>Most often this will be bug reports, which should generally be high priority for   the next release (or a hotfix to production if <code>critical</code> priority).</li> <li> <p>The PO / Tech Lead should triage tasks, assign priorities, and group into milestones   or releases.</p> <p>See the issue labels section for details about priority assignment.</p> </li> </ul>"},{"location":"dev-practices/#setting-priorities","title":"Setting Priorities","text":"<ul> <li>Generally bugs will take priority over new features, particularly those blocking   the users workflow.</li> <li>Milestones for a tool will be assessed based on internal team discussion, in   addition to public Slack and Git-based discussion.</li> <li>The determined priorities and milestones will be a tradeoff between:<ul> <li>Original vision and goals of the tool.</li> <li>Most prominent user requests.</li> <li>Global events and crisis response (activations).</li> <li>Requirements from actively engaged stakeholders / organization.</li> </ul> </li> </ul> <p>In future, we would like to better involve the public via a technical steering commitee or similar.</p>"},{"location":"dev-practices/#keeping-users-updated","title":"Keeping Users Updated","text":"<p>Users can keep informed about ongoing tasks in various ways:</p> <ul> <li>Public announcements on Slack / blog posts on HOT's   website, OSM Diaries, Dev.to, or other platforms.</li> <li>Releases on Github and included release notes.</li> <li>The overarching roadmap, showing progress for milestones and releases.</li> <li>Progress towards specific milestones, for new features.</li> <li>The task tracker for individual issue progress (particularly   for bugs).</li> <li>Any discussions around particular features that have not been agreed   upon and translated into an issue / milestone.</li> </ul>"},{"location":"dev-practices/#questions-clarification","title":"Questions / Clarification","text":"<ul> <li>HOT works fully in the open and is accountable for being clear about changes   made to our tools.</li> <li> <p>If clarity is poor about what the vision of our tools are, features we are planning,   how we work, when requirements will be met, or anything else, then we have open   channels of communication:</p> <ul> <li>Via our public Slack, either in channels related to a tool, or as a private message to a member of the tech team. </li> </ul> <ul> <li>Via Github issues or discussions. </li> </ul> <ul> <li>Direct message or email to the Product Owner or Tech Lead of the tool in question.</li> <li>In the monthly 'Open Tech and Innovation Space' calls advertised on Slack.</li> <li>In future, we also hope to have something akin to a 'Technical Steering Commitee',   in which any tech-facing member of the public is welcome to provide input.</li> </ul> </li> </ul>"},{"location":"dev-practices/#defining-how-we-build","title":"Defining How We Build","text":""},{"location":"dev-practices/#git-repo-management","title":"Git Repo Management","text":"<p>Management of tasks and code via Github.</p>"},{"location":"dev-practices/#branches","title":"Branches","text":"<ul> <li>Generally our repos will have three key branches:<ul> <li>dev: the ongoing development to which PRs are made.</li> <li>stage: optional branch where new features pass additional testing   stages prior to deployment to the main website.</li> <li>main: the code that is currently deployed to the main website.</li> </ul> </li> </ul>"},{"location":"dev-practices/#issues-features","title":"Issues \\ Features","text":""},{"location":"dev-practices/#issue-tags","title":"Issue Tags","text":"<ul> <li>Label / tag issues where appropriate, for example as <code>backend</code> or <code>frontend</code>.</li> </ul> <ul> <li>Priorities can be assigned:.<ul> <li><code>priority:critical</code>: blocking current tasks or user workflow.</li> <li><code>priority:high</code>: should be addressed as a priority.</li> <li><code>priority:low</code>: backlog of tasks that will be addressed in time.</li> </ul> </li> </ul> <ul> <li>Difficulty can be estimated (and may not be accurate):<ul> <li><code>effort:low</code>: small task, likely a few hours.</li> <li><code>effort:medium</code>: larger task, may take a day or two.</li> <li><code>effort:high</code>: a broader scope task with unclear timeline. <p>Ideally there should not be many tasks with <code>effort:high</code>. If there are, consider breaking them down to smaller tasks.</p> </li> </ul> </li> </ul> <ul> <li>The testing status can be tracked:<ul> <li><code>testing:ready</code>: the issue has been fixed and is ready to test.</li> <li><code>testing:fail</code>: the issue was not fixed as intended and requires additional   work from the developer.</li> <li>The <code>testing:xx</code> label can be removed when an issue is closed. <p>Note that <code>testing</code> is used as a simpler alternative to project management terminology such as <code>quality control</code> or <code>quality assurance</code>.</p> </li> </ul> </li> </ul> <ul> <li>Issues marked <code>good first issue</code> are approachable to newcomers in the repo.<ul> <li>Ideally the repo should always have a few low priority <code>good first issue</code>   tags to help foster open source contribution / onboarding.</li> </ul> </li> </ul> <p>Note</p> <p>To attach labels to an issue, the Github user requires at least <code>triage</code> level permission for the repo.</p> <p>This can be tracked at a repo level with a <code>contributors</code> group, including those that have contributed regularly to include them in the development flow.</p>"},{"location":"dev-practices/#issue-assignment","title":"Issue Assignment","text":"<ul> <li>Assign issues to the dev who will work on it.</li> <li>Issues can be assigned in advance if the devs is known.</li> <li>Devs can self-assign tasks.</li> </ul>"},{"location":"dev-practices/#writing-merging-code","title":"Writing &amp; Merging Code","text":"<p>PR = pull request, merge request, or similar terminology simply meaning to merge code!</p>"},{"location":"dev-practices/#pr-drafts","title":"PR Drafts","text":"<ul> <li>Create a draft PR for works in progress.</li> <li>Push as early as possible to draft, especially if there is a chance you   may get sidetracked on other work (so another dev could feasibly pick   up where you left off).</li> </ul>"},{"location":"dev-practices/#pr-assignment","title":"PR Assignment","text":"<ul> <li>Assign yourself if you are working on the issue.</li> <li>Assign another dev if you need to pass off the development to them.</li> <li>The re-assigned dev can then assign the task back to the original dev for   validation.</li> </ul>"},{"location":"dev-practices/#pr-review","title":"PR Review","text":"<ul> <li>Any developer can review a PR, as long as one dev reviews prior to   merge.</li> <li>In addition to a sanity check, a review should ideally be a technical   ideally suggesting feedback where code could be reused, best practices that   should be used, etc.</li> <li>Assign devs for review - frontend can review backend and vice versa.</li> <li>Once review is complete and the PR is out of draft state, then any dev can   merge.</li> </ul> <p>Note</p> <p>Once a PR is merged, the merged should ensure that the <code>testing:ready</code> label is applied to the relevant issue, if required.</p>"},{"location":"dev-practices/#linking-issues","title":"Linking Issues","text":"<ul> <li>Issues can be linked via either:<ul> <li><code>fixed #123</code> syntax to automatically close the issue on merge.</li> <li><code>related #123</code> syntax to allow the Product Owner / Manager close the issue   manually once they are satisfied the issue is resolved fully.</li> </ul> </li> <li>Generally the second approach is preferred</li> </ul>"},{"location":"dev-practices/#marking-issues-as-solved","title":"Marking Issues As Solved","text":"<ul> <li>Once merged, code should be deployed automatically from the <code>dev</code> branch   to the <code>dev</code> server.</li> <li>This allows for the Project Owner / Manager to thoroughly test the changes   and either close the issue, or add the <code>testing:fail</code> tag.</li> <li>Once closed, this will be reflected on the milestone percentage, roadmap   progress, and task board complete tasks.</li> </ul> <p>Note</p> <p>Note the distinction here between the technical code review and the the review of the solution from a user perspective via testing.</p>"},{"location":"dev-practices/#project-management","title":"Project Management","text":"<p>Still related to Github, but describing higher level project management and project direction.</p>"},{"location":"dev-practices/#milestones","title":"Milestones","text":"<ul> <li>Encapsulates a set of issues into a logical bigger task, with or without   an assigned deadline.</li> <li>Discussed in team meetings and decided on via priorities.</li> <li>Project Owner (PO) decides on priorities, tech lead decides on which tasks   are required to achieve that goal.</li> <li>Milestones are grouped into a release, which has a set deadline.</li> </ul>"},{"location":"dev-practices/#releases","title":"Releases","text":"<ul> <li>A release encompasses multiple milestones, plus additional bugfixes   and minor improvements.</li> <li>Releases are tracked on the roadmap and have an approximate deadline.</li> <li>Ideally releases should be around once a month at minimum, to regularly   deliver incremental updates to the user (~agile project management).</li> </ul>"},{"location":"dev-practices/#roadmaps","title":"Roadmaps","text":""},{"location":"dev-practices/#technical-roadmap","title":"Technical Roadmap","text":"<ul> <li>Higher level roadmap based on releases and milestones.</li> <li>Managed on Github, linked in the README.</li> <li>The roadmap should include:<ul> <li>Milestones (with optional dates) and issues linked to releases.</li> <li>Releases labelled over the top, showing the anticipated next release date.</li> </ul> </li> </ul> <p>Example (from FMTM):</p> <p></p>"},{"location":"dev-practices/#user-roadmap","title":"User Roadmap","text":"<ul> <li>Most users do not want to delve into Github roadmaps and issues.</li> <li>A simpler user-centric roadmap can be written in simple Markdown   table syntax.</li> <li>The table is a chronological timeline, divided into three categories:<ul> <li>\u2705 for done tasks</li> <li>\u2699\ufe0f for ongoing tasks</li> <li>[no emoji] for upcoming tasks</li> </ul> </li> </ul> <p>Example user roadmap (from Drone TM):</p> <p></p> <p>Note</p> <p>Previously we used to use a three-column table with categories: - In Progress - Next - Future</p> <p>However after agreeing as a team, we decided to streamline the approach to be less verbose, and to better show the ordering of priorities, as above.</p> <p>The user-story based wording was also partially replaced using emojis for the context of each feature (for brevity when used in a README file / front page).</p>"},{"location":"dev-practices/#task-board","title":"Task Board","text":"<ul> <li>An optional stage to easier visualise developer time.</li> <li>Should be as automated as possible:<ul> <li>Issues added to a project are added to the backlog.</li> <li>Issues assigned to a dev are moved to 'In Progress'.</li> <li>Issues labelled with <code>QA Ready</code> are moved to 'Review'.</li> <li>Issues completed should be moved to 'Complete'.</li> </ul> </li> <li>By automating this processes, the developer does not have duplicated work.</li> <li>Ideally we can keep track of which developer is working on   what task.</li> <li>This becomes especially important when we also have outside collaborators.</li> </ul> <p>Example:</p> <p></p>"},{"location":"dev-practices/#discussions","title":"Discussions","text":"<ul> <li>To discuss more general topics in the public, so that anyone can contribute.</li> <li>Partly used to document the design decisions we have taken.</li> <li>Tag people specially for input, as it makes them more likely to add ideas.</li> <li>We can also use these for the staging server tests on each release cycle,   e.g. https://github.com/hotosm/field-tm/discussions/1335</li> </ul>"},{"location":"dev-practices/#release-notes","title":"Release Notes","text":"<ul> <li>No technical details, move those to a dropdown in markdown:</li> </ul> <p>Example:</p> <pre><code>```md\n&lt;details&gt;\n  &lt;summary&gt;Technical Summary&lt;/summary&gt;\n    * Add healthcare form category &amp; minor fixes by @spwoodcock in https://github.com/hotosm/field-tm/pull/1555\n    * Fix/requested page redirection by @NSUWAL123 in https://github.com/hotosm/field-tm/pull/1559\n    * Test coverage for update project route by @azharcodeit in https://github.com/hotosm/field-tm/pull/1557\n&lt;/details&gt;\n```\n</code></pre> <p></p> <ul> <li>Higher level info on bugs fixed, new features added, things improved.</li> <li>Add screenshots throughout.</li> </ul>"},{"location":"dev-practices/#deployment-flow","title":"Deployment Flow","text":"<p>These stages go in order, from local development, through to production deployment.</p>"},{"location":"dev-practices/#local-development","title":"Local Development","text":"<ul> <li>Devs develop features on their local instance.</li> <li>Use <code>docker-compose.yml</code> setup for testing.</li> <li>Once feature and testing complete, make a PR to the <code>dev</code> branch.</li> </ul>"},{"location":"dev-practices/#development-deployment","title":"Development Deployment","text":"<ul> <li>Once a PR is approved, it is merged to <code>dev</code>.</li> <li>This triggers a workflow to automatically deploy the code changes on the dev server.</li> <li>The purpose of this stage is for:<ul> <li>Fast CI, i.e. the developer sees their code in action quickly.</li> <li>Easy QA tests by the project manager on the dev server.</li> </ul> </li> </ul>"},{"location":"dev-practices/#staging-deployment","title":"Staging Deployment","text":"<ul> <li>The purpose of this step is to regularly release versions of the software that   power users (and the project owners) can test.<ul> <li>Anyone who doesn't mind occasional breakage is welcome to use this server publicly.</li> </ul> </li> <li>At a set interval (approx bi-weekly), the updates made on <code>dev</code> are merged into   <code>staging</code> for feature stabilisation.<ul> <li>This can be done via PR, although sometimes there may be merge conflicts to resolve.</li> <li>Alternatively, the branch can be reset to the latest <code>dev</code> and built upon:   <code>git reset --hard origin/dev</code></li> </ul> </li> <li>Once merged, the functionality is thoroughly tested and patched (if required).<ul> <li>Patches can either me made on <code>dev</code> and merged into <code>staging</code>.</li> <li>Or be made directly to <code>staging</code> if there branches have diverged significantly.</li> </ul> </li> <li>Once approved, the <code>staging</code> branch auto-deploys to the staging server.</li> </ul>"},{"location":"dev-practices/#production-deployment","title":"Production Deployment","text":"<ul> <li>The staging server instance is thoroughly tested by the product owner, and   bug reports filed.</li> <li>The release is hardened into longer interval (approx bi-monthly) production releases.</li> <li>A PR is made from <code>staging</code> to <code>main</code> branch.</li> <li>Once approved and the code merged, a Github release is made.</li> <li>A release is available on Github, including all relevant release notes for   what has been updated.</li> <li>The release will trigger the workflow to deploy to the production server.</li> </ul>"},{"location":"dev-practices/#hotfixes","title":"Hotfixes","text":"<ul> <li>If an issue is found after a production release is made, a hotfix can be used to   patch the production code.</li> <li>There are two methods to do this:<ul> <li>If the <code>dev</code> and <code>main</code> branches have diverged significantly, the hotfix can   be made as a PR directly to <code>main</code>, then reconciled later with <code>dev</code>.</li> <li>Otherwise, the fix can be made as a PR to <code>dev</code>, then <code>cherry-picked</code> upstream   through <code>staging</code> then to <code>main</code>.</li> </ul> </li> </ul>"},{"location":"dev-practices/#security-vulnerabilities","title":"Security Vulnerabilities","text":"<ul> <li>Vulnerabilities in software are an inevitability and may be reported by   users, developers, or automated CI tools.</li> <li>They will typically be referenced by a Common Vulnerabilities and Exposures   (CVE) reference ID, which can be looked up in various CVE tracking websites.</li> <li>Once a vulnerability is reported and verified, actions can be taken:<ul> <li>System dependencies: generally updating a container image version,   or rebuilding the image should fix these, as vendors update regularly.</li> <li>Package dependencies: an issue in your bundled sub-dependencies   may be fixed by updating the package version to the latest. CVEs are   generally fixed quickly by the package maintainer.</li> <li>In our repo code: these may be picked up by static and dynamic code   scanners and are generally fixed as part of the pre-commit or PR   review process (CI workflows).</li> </ul> </li> <li>Fixes should be pushed through to production as soon as possible, as a   <code>hotfix</code> branch including the updated image/package version or code.</li> </ul>"},{"location":"dev-practices/#other-feature-demo-releases","title":"Other: Feature Demo Releases","text":"<ul> <li>A feature demo release is a throwaway instance of the tool with a particular purpose.</li> <li>Functionality is developed here for various reasons:<ul> <li>Specific updates for a single project that won't be used elsewhere.</li> <li>Very fast updating of the server, without interfering with the typical release   flow.</li> </ul> </li> <li>The key point is that these branch instances are single use and will be   shut down once the project has ended.</li> <li>The easiest approach is probably to:<ul> <li>Create and login to a server.</li> <li>Clone the repo and checkout to the feature branch <code>feature-demo/some-feature</code>.</li> <li>Run the commands to build and run the tool.</li> </ul> </li> <li>Alternatively, a workflow can be made to auto-deploy:<ul> <li>Triggering on a branch naming convention: <code>feature-demo/some-feature</code>.</li> <li>The user will have to enter an SSH key into the Gitlab secrets.</li> <li>The workflow will deploy to the server remotely when the branch is pushed to.</li> <li>This approach is less preferred, as the user requires write access to the   Github repo.</li> </ul> </li> </ul>"},{"location":"dev-practices/#note-about-linear","title":"Note About Linear","text":"<ul> <li>The information above is due to an overhaul!</li> <li>We have decided to use Linear as a project management tool,   allowing for easier thematic linking of tasks across projects.</li> <li>The Linear tool itself is only accessible by HOT staff, but   absolutely everything there is synced publicly and bi-directionally   to Github!</li> </ul>"},{"location":"dev-practices/#issue-management","title":"Issue management","text":"<ul> <li>Issues: lowest level, per repo.<ul> <li>Each issue should have a label <code>repo:repo-name</code> to allow for   easier sorting in Linear (a small workaround to ensure we   remain on the 'free' tier).</li> <li>Issues and all info are synced automatically from Github,   but the only manual step we need is adding the mentioned   label.</li> <li>Developers should interact and comment on Github issues.</li> <li>Managers can track progress, group issues, and make   dashboards via Linear.</li> <li>\ud83d\udca1 Example: Add additional endpoints to backend for gathering   user stats on a dashboard.</li> </ul> </li> <li>Milestones: these are an optional part of the workflow   capturing multiple issues that have a specific deadline and   purpose, e.g. linked to a specific contract.<ul> <li>\ud83d\udca1 Example: Add badges to user profile, based on stats   generated in each mapping category. Due 29/10/2025.</li> </ul> </li> <li>Projects: a grouping of issues and milestones around a   particular theme, i.e. essentially an epic in other tools.   These can include issues from multiple repos, if they tie   together.<ul> <li>\ud83d\udca1 Example: Super Mapper feature in Tasking Manager.</li> </ul> </li> <li>Initiatives: a very high level goal we wish to achieve,   encompasing multiple projects.<ul> <li>\ud83d\udca1 Example: Better user engagement by providing useful   stats and better tracking of mapping progress across   projects.</li> </ul> </li> </ul> <p>Note</p> <p>To add the <code>repo:repo-name</code> automatically for each new issue, we can create the following Github workflow</p> <pre><code># We add a label `repo:repo-name` to each new issue,\n# for easier tracking in external systems\n\nname: \ud83c\udff7\ufe0f Issue Label\n\non:\n  issues:\n    types:\n      - opened\n\njobs:\n  issue-label:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n\n    steps:\n      - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          GH_REPO: ${{ github.repository }}\n          NUMBER: ${{ github.event.issue.number }}\n          LABELS: repo:drone-tm\n</code></pre>"},{"location":"dev-practices/#public-roadmap","title":"Public roadmap","text":"<ul> <li>We have a public roadmap linking all tools together under   https://hotosm.steelsync.io/tech-roadmap</li> <li>This is made using a tool called SteelSync (paid unfortunately,   but cheap), that gathers data from the Linear API and easily   creates a public dashboard.</li> <li>We chose the approach to save the overhead of developing   our own roadmap page using the Linear API.</li> </ul>"},{"location":"dev-practices/#feature-requests","title":"Feature requests","text":"<ul> <li>Technical users can still make issues and bug reports via   each Github repo.</li> <li>For general user requests (public, or internal to HOT),   we can all use the 'Request' button available on the   mentioned public roadmap page above.</li> <li>Each request will create an issue in Linear. The tech team   will triage and add the issue to the relevant Github code   repository, then place within a project / initative with   timelines.</li> </ul>"},{"location":"diagrams/","title":"How does everything fit together?","text":"<p>The HOT Tech Team is working to improve the user experience across multiple tools, and working towards an end to end data flow between projects.</p> <p>During disaster response, or a humanitarian mapping campaign, a project manager or mapper may require:</p> <ul> <li>High quality base imagery.</li> <li>Digitized map features, derived from this imagery.</li> <li>Field-verified information about these features.</li> <li>Data exported in a useful format for further analysis.</li> </ul> <p>The goal of our proposed E2E Flow is to fill in these gaps to aid an effective response.</p>"},{"location":"diagrams/#e2e","title":"E2E","text":""},{"location":"diagrams/#1-imagery-collection","title":"1. Imagery Collection","text":""},{"location":"diagrams/#2-digitization","title":"2. Digitization","text":""},{"location":"diagrams/#block-model-diagram","title":"Block Model Diagram","text":""},{"location":"diagrams/#overall-architecture","title":"Overall Architecture","text":""},{"location":"diagrams/#old-hot-ecosystem-digram-2023","title":"Old HOT Ecosystem Digram (2023)","text":""},{"location":"modules/","title":"Modularization","text":"<p>HOT has many tools which have similar requirements for backend and frontend functionality. To reduce long-term maintainance and code duplication, it's better to have the shared functionality in standalone modules. The other advantage of small sharable modules is it's much easy to enhance or debug when not buried in much larger prohects.</p>"},{"location":"modules/#backend","title":"Backend","text":"<p>Backend Python modules are released on PyPi to be installed across multiple services.</p> <p>See the backend page for more info.</p> <p>Here's a recent Presentation that covers the backend modules in more detail.</p>"},{"location":"modules/#frontend","title":"Frontend","text":"<p>As a form of standardization, we use React as our frontend framework.</p> <p>Many services have common UI components that can be shared (headers, buttons, sidebars, etc).</p> <p>We also use frontend map libraries extensively (obviously).</p> <p>Currently we favour OpenLayers due to it's breadth of functionality.</p> <p>See the frontend page for more info.</p>"},{"location":"decisions/","title":"Architectural Decisions","text":"<p>Markdown Architectural Decision Records documenting the technical decision taken across HOTOSM projects.</p> <p>These decisions are advisory, with some scope for variation in projects if specific requirements arise.</p>"},{"location":"decisions/0001-draw-io/","title":"Use draw.io (diagrams.net) for technical diagrams","text":""},{"location":"decisions/0001-draw-io/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need a simple and intuitive way to make technical diagrams.</p> <p>This encompasses drawing flow charts of various forms, architectural diagrams, and other diagrams showing the relationships between technical components.</p>"},{"location":"decisions/0001-draw-io/#considered-options","title":"Considered Options","text":"<ul> <li>UML, drawn via LibreOffice</li> <li>Diagrams as code</li> <li>Diagrams as markdown</li> <li>Simple SVG editing software, such as Inkscape</li> <li>Diagrams.net, a wrapper for draw.io</li> </ul>"},{"location":"decisions/0001-draw-io/#decision-outcome","title":"Decision Outcome","text":"<ul> <li>The author deemed UML to be useful for certain circumstances, but   largely an academic exercise. It shines at creating relationship   diagrams between Object-oriented code, which is not what we need.   It also looks pretty bad when public facing.</li> <li>Tools like Diagrams and Mermaid could be great to use in certain   instances (Mermaid is probably favoured for the simplicity of   markdown). However, they are still not very approachable if   a non-technical user wishes to edit these diagrams.</li> <li>Diagrams.net (draw.io) has many advantages and hence was chosen:</li> </ul>"},{"location":"decisions/0001-draw-io/#consequences","title":"Consequences","text":"<ul> <li>Good, because draw.io uses an open XML standard to save, plus can be exported in   SVG or PNG format as needed</li> <li>Good, because it's super simple to use, particularly if exported to SVG for   editing. Anyone should be able to edit the diagrams.</li> <li>Good, because draw.io is open-source, with a permisive license even for   commercial use.</li> <li>Good, because the diagrams.net wrapper has Git integration where you save straight   to a repo, meaning we can auto-deploy docs when diagrams are edited.</li> <li>Bad, it is slightly more manual with the design / placement than code --&gt; diagram   options such as Diagrams or Mermaid.</li> <li>Bad, because diagrams.net could decide to shut down or change their license.   But this would not be a huge deal due to open standards / easy migration.</li> </ul>"},{"location":"decisions/0002-mkdocs/","title":"Use mkdocs for technical documentation","text":""},{"location":"decisions/0002-mkdocs/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need to both:</p> <ul> <li>Display written technical documentation.</li> <li>Generate technical API docs automatically.</li> </ul>"},{"location":"decisions/0002-mkdocs/#considered-options","title":"Considered Options","text":"<ul> <li>Vanilla MKDocs</li> <li>MKDocs with mkdocs-material theme</li> <li>Vanilla Astro</li> <li>Astro Starlight</li> <li>Manual HTML / JS / CSS</li> <li>Any other web framework</li> </ul>"},{"location":"decisions/0002-mkdocs/#decision-outcome","title":"Decision Outcome","text":"<p>We chose MKDocs with mkdocs-material theme as an excellent open-source tool with a large community backing it.</p> <p>Many of the other options either have too high of a maintenance burden, or don't integrate as nicely with our code.</p> <p>Astro Starlight is a close contender, as it's an equally good markdown renderer, with probably nicer themeing. This decision was taken prior to full knowledge and testing of Astro Starlight, so it was not given as fair of a chance. Previous experience with a tool counts for something, and more devs at HOT had knowledge of mkdocs. It's also a Python tool, when most of our backend code is written in Python, giving the option for auto-generating API docs.</p>"},{"location":"decisions/0002-mkdocs/#consequences","title":"Consequences","text":"<ul> <li>Good, because markdown is super simple, even for non-technical users, broadening   the possible number of users that could contribute (not just devs).</li> <li>Good, because mkdocs-material does most of the work for us of making the docs   look half decent.</li> <li>Good, because it's a Python tool at heart, so we can use plugins like   <code>mkdocstrings-python</code> to very easily read docstrings and type hints to   auto-generate API documentation for us.</li> <li>Bad, because less customisation than would be possible with other solutions.   The docs may look slightly generic and less appealing.</li> </ul>"},{"location":"decisions/0003-react/","title":"Use React as our main frontend library","text":""},{"location":"decisions/0003-react/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>At HOTOSM we mostly develop web applications, which require a frontend component.</p> <p>We need to balance:</p> <ul> <li>Complexity / learning curve.</li> <li>Existing knowledge of staff.</li> <li>Knowledge of key contractors for various tools.</li> <li>Performance.</li> <li>Existing library support (community &amp; ecosystem).</li> <li>Scalability &amp; maintainability.</li> </ul>"},{"location":"decisions/0003-react/#considered-options","title":"Considered Options","text":"<ul> <li>Angular</li> <li>Vue</li> <li>React</li> <li>Svelte</li> <li>Solid</li> <li>Qwik</li> <li>Vanilla JS + Web Components</li> </ul>"},{"location":"decisions/0003-react/#decision-outcome","title":"Decision Outcome","text":"<p>This decision was made many years ago (~2018) with the introduction of Tasking Manager v3.</p> <p>The decision was to use React, as that is where the industry was centered around at that time.</p> <p>Due to technical debt, skills / knowledge with React within the team, and with various contractors, this decision has been carried forward into future projects, in an attempt to standardize:</p> <ul> <li>The learning curve / pool of experience.</li> <li>Speed and ease of setting up new projects.</li> <li>Avoiding the churn of frontend libraries: contractors must use React.</li> </ul> <p>This decision still stands as of 2025, however, it is becoming increasingly apparent that this should be revisited in the near future.</p> <p>There is also the higher level discussion for if we need a frontend framework at all, as much of our functionality could be implemented with simpler HTML-first technologies such as HTMX, with progressive enhancement.</p> <p>As a result, some flexibility is possible to choose more suitable tools for newly started projects.</p>"},{"location":"decisions/0003-react/#consequences","title":"Consequences","text":"<ul> <li>Good, the many people have React experience. It broadens the pool of available   contractors, and fits well with existing skills in the team.</li> <li>Bad, because performance is superceded by frameworks that do not rely on the   virtual-DOM, such as Svelte and Solid.</li> <li>Bad, because React is not as user friendly as it claims to be. It's far easier   to write bad React code, in comparison to frameworks like Svelte.</li> <li>Bad, because this unfriendliness to new developers does not only affect performance,   but also extends to the code complexity and ease of understanding / onboarding.</li> <li>Bad, because we don't necessarily need a web framework for all our tools!   We could probably build more resilient code using HTML with progressive enhancement.</li> </ul> <p>Note</p> <p>With JavaScript being the language of web browsers, this page makes the assumption that TypeScript is the general consensus best language to be developing frontend software with.</p> <p>However, do note that there are alternative approaches to typing, such as JSDocs, but the goal of type safety is the same regardless.</p>"},{"location":"decisions/0004-python/","title":"Use Python as our main backend language","text":""},{"location":"decisions/0004-python/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>At HOTOSM we mostly develop:</p> <ul> <li>Web applications.</li> <li>Data processing pipelines.</li> </ul> <p>We need a backend language that is:</p> <ul> <li>Beginner friendly, allowing for easy community contribution.</li> <li>Simply and quick to develop with, making us responsive when needed.</li> <li>Reasonably performant.</li> <li>General purpose, allowing us to achieve lots with the same skillset.</li> <li>Good existing library support, reducing code duplication.</li> </ul>"},{"location":"decisions/0004-python/#considered-options","title":"Considered Options","text":"<ul> <li>Python</li> <li>Golang</li> <li>Rust</li> <li>C++</li> <li>JavaScript</li> <li>Java / Kotlin</li> </ul>"},{"location":"decisions/0004-python/#decision-outcome","title":"Decision Outcome","text":"<p>Python was chosen as the best compromise across all languages listed.</p> <p>JavaScript would have the benefit of keeping our stack to a single language, however, it is doesn't have great performance. It is also more complex than Python, particularly when it comes to asynchronous programming.</p> <p>Rust and C++ are too low-level and complex for our use cases (unless a specific performance critical need arises).</p> <p>Golang is an excellent beginners language, however, may be slightly too low-level to recommend it to everyone (requiring knowledge of pointers etc). It's a good choice for specific cases, but library support is also lacking.</p> <p>Java is popular because of historic use, and is still arguably a good language, however it's pros do not outweight the cons when compared to more modern languages such as Golang or Rust.</p>"},{"location":"decisions/0004-python/#consequences","title":"Consequences","text":"<ul> <li>Good, the many people have Python experience. It broadens the pool of available   contractors, and fits well with existing skills in the team.</li> <li>Good, because it's the most versatile language listed as of 2025, including   excellent support for Web APIs, Machine Learning libraries, backend processing.</li> <li>Good, because it's very intuitive and simple to learn, often being a great first   language to learn for junior developers.</li> <li>Good, because of a huge amount of existing libraries in Python, meaning we have   to code less!</li> <li>Bad, because we use separate languages for frontend / backend development.</li> <li>Bad, it's not the most performant language of the bunch, particularly due to the   Global Interpreter Lock.</li> <li>Bad, because far from perfect dependency management (which might be solved   with the advent of https://github.com/astral-sh/uv)</li> </ul>"},{"location":"decisions/0005-github/","title":"Use Github as both our code repository &amp; CI/CD","text":""},{"location":"decisions/0005-github/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need somewhere to</p> <ul> <li>Store our code to work on it collaboratively.</li> <li>Raise issues and manage documentation around the code.</li> <li>Track development via roadmaps and task trackers.</li> </ul> <p>We also need a pipeline runner (CI/CD):</p> <ul> <li>Create build artifacts for deployment.</li> <li>Run automated testing.</li> <li>Deploy our code into various development environments.</li> </ul>"},{"location":"decisions/0005-github/#considered-options","title":"Considered Options","text":"<ul> <li>Github</li> <li>Gitlab</li> <li>Codeberg</li> <li>Bitbucket</li> </ul>"},{"location":"decisions/0005-github/#decision-outcome","title":"Decision Outcome","text":"<p>Github was originally chosen as it provided many benefits for free, over the competition.</p> <p>Cost and simplicity are key factors for us, being a small NGO that receives project specific funding.</p> <p>Since the decision was taken, Github was acquired by Microsoft and has some practices that we do not agree with (license violations for Copilot, amongst others).</p> <p>Reluctantly, we are stuck with Github due to the migration effort that would be required.</p>"},{"location":"decisions/0005-github/#consequences","title":"Consequences","text":"<ul> <li>Good, because free to use, including uncapped CI/CD usage as an NGO.</li> <li>Good, has the most visibility and reach of all other platforms.</li> <li>Bad, because as an organization it undertakes many practices we do not agree with.</li> <li>Bad, the more we use Github workflows (propriatary), the further we are   vendor-locked.</li> <li>Bad, because it would be difficult to migrate all of the accumulated issues,   milestones, project boards, subissues, etc.</li> </ul>"},{"location":"decisions/0006-slack/","title":"Use Slack as our primary team and community chat platform","text":""},{"location":"decisions/0006-slack/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need a platform for:</p> <ul> <li>Day-to-day team communication and asynchronous coordination.</li> <li>Community engagement and support.</li> <li>Integration with project tooling (e.g., CI/CD notifications, GitHub activity).</li> <li>Easy onboarding for new collaborators and contributors.</li> </ul> <p>Our ideal platform would also:</p> <ul> <li>Be accessible to external contributors.</li> <li>Support persistent message history.</li> <li>Be open and interoperable, with low risk of vendor lock-in.</li> <li>Respect user privacy and data sovereignty.</li> </ul>"},{"location":"decisions/0006-slack/#considered-options","title":"Considered Options","text":"<ul> <li>Slack</li> <li>Zulip</li> <li>Mattermost</li> <li>Rocket.Chat</li> <li>Matrix / Element</li> </ul>"},{"location":"decisions/0006-slack/#decision-outcome","title":"Decision Outcome","text":"<p>Slack was adopted early in our organizational growth, largely because:</p> <ul> <li>Many existing and incoming collaborators were already on Slack.</li> <li>It provided a smooth UX with strong mobile and desktop support.</li> <li>NGO discounts made the Pro plan affordable, allowing for full message history and   integrations.</li> </ul> <p>However, as we scale and become more conscious of long-term sustainability, Slack's proprietary nature and closed ecosystem present issues.</p> <p>Migrating to an open platform would align better with our values, but the migration cost and risk of fracturing our community are currently too high.</p> <p>In 2025, we will be revisiting the use of Slack and possible open source alternatives (as Slack changed their subscription model and will be costly to maintain with many users).</p>"},{"location":"decisions/0006-slack/#consequences","title":"Consequences","text":"<ul> <li>\u2705 Good, because our community is already established on Slack.</li> <li>\u2705 Good, because integrations (e.g., with GitHub, monitoring tools, deployment   notifications) are mature and easy to configure.</li> <li>\u2705 Good UX, especially for newcomers, thanks to wide adoption and familiar UI.</li> <li>\u274c Bad, because Slack is a proprietary, closed platform with limited options   for self-hosting or data ownership.</li> <li>\u274c Bad, because the free tier is too limited (e.g., no full message history),   making us reliant on discounted paid plans.</li> <li>\u274c Bad, migration to open alternatives (e.g., Zulip or Matrix) would be technically   possible but socially risky, as many users may not follow or adapt easily.</li> <li>\u274c Bad, limited support for open standards and federation restricts long-term   resilience.</li> </ul>"},{"location":"decisions/0007-pnpm/","title":"Use <code>pnpm</code> as our frontend dependency manager","text":""},{"location":"decisions/0007-pnpm/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>In our frontend projects, we need a reliable, efficient, and developer-friendly package manager for JavaScript/TypeScript dependencies.</p> <p>Key concerns include:</p> <ul> <li>Speed and disk usage efficiency.</li> <li>Deterministic and reproducible installs.</li> <li>Compatibility with existing tools and workflows.</li> <li>Minimizing node_modules bloat and install conflicts.</li> </ul>"},{"location":"decisions/0007-pnpm/#considered-options","title":"Considered Options","text":"<ul> <li>npm</li> <li>yarn (classic or berry)</li> <li>pnpm</li> </ul> <p>Options not considered: package managers used by runtimes other than NodeJS, such as Deno or Bun.</p>"},{"location":"decisions/0007-pnpm/#decision-outcome","title":"Decision Outcome","text":"<ul> <li>pnpm was selected as our default package manager due to its performance benefits,   disk space efficiency, and strict dependency resolution.</li> <li>Compared to npm and yarn, pnpm uses a content-addressable storage system and hard   links to avoid duplication.</li> <li>It also enforces more consistent dependency resolution, helping to catch errors   early in development.</li> </ul>"},{"location":"decisions/0007-pnpm/#consequences","title":"Consequences","text":"<ul> <li>\u2705 Fast install speeds, especially in CI environments.</li> <li>\u2705 Reduced disk usage thanks to shared dependency storage.</li> <li>\u2705 More deterministic builds with stricter dependency isolation.</li> <li>\u274c Less familiar for some contributors compared to npm/yarn.</li> <li>\u274c Occasional compatibility issues with legacy packages expecting a flat node_modules.</li> </ul>"},{"location":"decisions/0008-uv/","title":"Use <code>uv</code> as our backend dependency manager","text":""},{"location":"decisions/0008-uv/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need a modern Python package management tool that is:</p> <ul> <li>Much faster than pip and pip-tools.</li> <li>Able to handle lock files and virtual environments cleanly.</li> <li>Better at resolving complex dependency trees reproducibly.</li> </ul>"},{"location":"decisions/0008-uv/#considered-options","title":"Considered Options","text":"<ul> <li>pip + virtualenv</li> <li>pipenv</li> <li>poetry</li> <li>pdm</li> <li>uv</li> </ul>"},{"location":"decisions/0008-uv/#decision-outcome","title":"Decision Outcome","text":"<p>uv (by Astral) was selected for its speed, strict reproducibility, and modern tooling ergonomics.</p> <p>It significantly improves dependency resolution times and offers a seamless developer experience.</p> <p>Unlike traditional tools like pip, uv performs rapid dependency resolution and installs, while producing lock files that ensure consistency across platforms and environments.</p>"},{"location":"decisions/0008-uv/#consequences","title":"Consequences","text":"<ul> <li>\u2705 Extremely fast dependency resolution and installation (Rust based).</li> <li>\u2705 Modern, opinionated tooling with strong defaults.</li> <li>\u2705 Simplifies management of virtual environments and lock files.</li> <li>\u2705 Support for workspace packages, to allow for easier monorepo setups.</li> <li>\u274c Less mature ecosystem and community support (as of 2025, growing fast!).</li> <li>\u274c Requires team familiarity with new workflows.</li> </ul>"},{"location":"decisions/0009-load-testing/","title":"Use k6 or Oha for application load testing","text":""},{"location":"decisions/0009-load-testing/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need lightweight tools to simulate real-world traffic for performance testing of our APIs and services.</p> <p>Requirements include:</p> <ul> <li>Easy setup and usage.</li> <li>Ability to run both quick, one-off benchmarks and complex scripted scenarios.</li> <li>CI/CD integration capability.</li> </ul>"},{"location":"decisions/0009-load-testing/#considered-options","title":"Considered Options","text":"<ul> <li>Apache JMeter</li> <li>Locust</li> <li>k6</li> <li>oha</li> <li>wrk</li> </ul>"},{"location":"decisions/0009-load-testing/#decision-outcome","title":"Decision Outcome","text":"<p>In reality, different tools cover different use cases.</p> <p>We adopted a hybrid approach using oha for simple benchmarks and k6 for more advanced, scripted load tests.</p> <p>oha is fast and easy to use for quick HTTP benchmarking (similar to wrk, but with a simpler interface and better reporting).</p> <p>k6 is a developer-centric load testing tool that supports JavaScript scripting for complex test scenarios, integrates well with CI pipelines, and produces meaningful metrics.</p> <p>Locust is also an acceptable choice if Python is preferred, CI-integration is not a priority, and complex user flows need to be simulated (such as authentication and other multi-step workflows).</p>"},{"location":"decisions/0009-load-testing/#consequences","title":"Consequences","text":"<ul> <li>\u2705 oha gives us a fast, no-frills tool to test latency and throughput of HTTP endpoints.</li> <li>\u2705 k6 allows scripting advanced load test scenarios using JavaScript, including   auth flows and dynamic data.</li> <li>\u2705 Both tools are CLI-friendly and CI-compatible.</li> <li>\u274c Splitting tools means learning two interfaces.</li> <li>\u274c k6 can be resource-intensive during heavy test scenarios.</li> </ul>"},{"location":"decisions/0010-k8s-domain/","title":"Use pattern convention for kubernetes namespace subdomains","text":""},{"location":"decisions/0010-k8s-domain/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need a universally applicable namespace for DNS resolution of kubernetes resources. Additionally, we should set a standard for all sub-domain naming for consistency across our tools.</p> <p>In this specific case, we are deciding on the DNS resolution for the new OpenAerialMap eoAPI, which will run alongside the old API for a time while we work on the uploader. So we will need to find a solution for the k8s setup that will affect all future tools while also accounting for the peculiarities of the initial setup.</p> <p>Our tools have:</p> <ul> <li>[project] which is an internal designator for project tracking</li> <li>[tool-name] which may be the same as [project] but can also designate a   specific tool within that project. Examples include <code>tasking-manager</code>,   <code>openaerialmap</code>, <code>fair-predictor</code>, and <code>eoapi</code></li> <li>[env] such as <code>dev</code>, <code>staging</code>, and <code>production</code></li> </ul> <p>In the Tasking Manager (running on ECS), we have:</p> <ul> <li>the frontend tasks.hotosm.org, tasks-dev.hotosm.org</li> <li>backend tasking-manager-production-api.hotosm.org, tasking-manager-dev-api.hotosm.org</li> </ul> <p>On the existing OAM we have:</p> <ul> <li>splash page openaerialmap.org</li> <li>frontend map.openaerialmap.org</li> <li>backend api.openaerialmap.org</li> </ul> <p>This decision will only affect the pattern of internal, predictable domains, not front-facing \"vanity\" ones that will be in use for production (eg. map.openaerialmap.org)</p>"},{"location":"decisions/0010-k8s-domain/#considered-options","title":"Considered Options","text":"<p>We considered the following schemes:</p> <ul> <li>[env].[tool-name].[cluster-namespace].hotosm.org (using multiple subdomains)</li> <li>[tool-name]-[env].[cluster-namespace].hotosm.org</li> <li>[env]-[tool-name].[cluster-namespace].hotosm.org</li> </ul> <p>For managing both the existing APIs temporarily, we can try to use an API gateway that resolved any option above to api.openaerialmap.org/v2/, however this would require migrating the existing old API to k8s as well. More simply, we could instead deploy the new API with a hotosm.org domain: oam-api.hotosm.org. Moving the service over to the hotosm.org domain will help unify our tools for our e2e goals. Eventually we could build the new frontend as oam.hotosm.org.</p>"},{"location":"decisions/0010-k8s-domain/#decision-outcome","title":"Decision Outcome","text":"<p>For the kubernetes namespace we will go with the following naming scheme:</p> <p>[tool-name]-[component]-[env].[cluster-namespace].[cluster-name].hotosm.org</p> <p>Our cluster name will be \"k8s-prod\" (including the environment), so the OAM eoAPI will be eoapi-backend-prod.imagery-services.k8s-prod.hotosm.org which will resolve to oam-api.hotosm.org.</p> <p>Cluster namespaces will use project tag values whenever possible. If not, tool tag value or a unique identifier will also work.</p> <p>The deepest subdomain <code>[toolname]-[component]-[env]</code> has some flexibility in naming for brevity and clarity of purpose. No need to specify <code>[tool-name]</code> if it's the same as the namespace, or <code>[env]</code> if there is never going to be a second environment. Some examples:</p> <ul> <li>odk-frontend-prod.field-tm.k8s-prod.hotosm.org</li> <li>api-dev.field-tm.k8s-prod.hotosm.org</li> <li>eoapi-backend-prod.imagery-services.k8s-prod.hotosm.org</li> <li>oam-uploader-dev.imagery-services.k8s-prod.hotosm.org</li> <li>fastapi-prod.tasking-manager.k8s-prod.hotosm.org</li> </ul> <p>Below is the list of accepted <code>project</code> and <code>tool</code> tags to be used for cluster namespace and tool naming. We also have a list of accepted environment tags: <code>dev</code>, <code>staging</code>, <code>production</code>, <code>demo</code>, and <code>testing</code>. Example components could be <code>api</code>, <code>frontend</code>, <code>backend</code>, <code>uploader</code>, <code>scheduler</code>, <code>worker</code>, <code>database</code>, or <code>storage</code>.</p> Project Tags Tool Tags imagery-services oam, marblecutter, eoapi tasking-manager tasking-manager, hot-dataservice fmtm fieldtm, odk fair, fAIr fair, fair-api, fair-predictor map-data-access raw-data-api, overture, export-tool, umap"},{"location":"decisions/0010-k8s-domain/#consequences","title":"Consequences","text":"<p>We will need to keep the old OAM API intact until we get uploader functionality into the new API. That will remain as api.openaerialmap.org.</p>"},{"location":"decisions/0011-sso-auth/","title":"Use Hanko for shared auth SSO solution across HOT apps","text":""},{"location":"decisions/0011-sso-auth/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need a form of shared authentication to:</p> <ul> <li>Have shared login between apps, so if I login into one HOT tool,   I am automatically logged into the others.</li> <li>Reduce code duplication across tools, where we have custom logic   to login via various OAuth providers. This is for both the   frontend and the backend code.</li> <li>Centralise OAuth config for a single app to connect to OSM / Google   etc, for easier management. No need to configure credentials for   every single app.</li> <li>Have a consistent login flow for all apps, making for better user   experience, and developer experience (we can bundle config in hotosm/ui).</li> <li>It would also be nice to integrate modern passwordless logins such as   passkeys with biometrics (bonus).</li> </ul> <p>Other requirements:</p> <ul> <li>An option to delete user profiles when requested.</li> <li>Easy way for self-hosters of our tool to include Hanko.</li> </ul>"},{"location":"decisions/0011-sso-auth/#considered-options","title":"Considered Options","text":"<ul> <li>Authelia</li> <li>Authentik</li> <li>Auth0</li> <li>Keycloak</li> <li>Hanko</li> </ul>"},{"location":"decisions/0011-sso-auth/#decision-outcome","title":"Decision Outcome","text":"<p>Auth0 is closed source. Keycloak is antiquated Java and too heavyweight.</p> <p>Authelia and Authentik are both similar tools, with a preference for Athelia, written in Golang and extremely simple to run + resource efficient.</p> <p>However Authelia is primarily focused around acting as auth for reverse proxied apps. Typically the entire web UI / interface is placed behind the reverse proxy, with authelia there to check if the user is authenticated. The request is intercepted and the users is redirected to login, then can access the app afterwards - this is not what we want.</p> <p>Instead, we need a true Identity Provider that issues and verifies tokens, which our APIs can validate independently. The IdP does not sit 'in front' of services like a reverse proxy \u2014 it just handles the login flow:</p> <ul> <li>The frontend is public.</li> <li>The user clicks to 'log in' and does an OAuth redirect flow.</li> <li>The user is returned to the web frontend, with a JWT set in a secure cookie.</li> <li>The cookie is sent to the API, verifying that the user is logged in.</li> </ul> <p>In order to delete a user profile, this is still done per-app database (not changed). The user info stored in the Hanko db from OAuth is minimal and does not need to be considered.</p> <p>As for self-hosting, we can provide the lightweight Hanko binary / container as part of the docker compose config for self-hosters to include (the binary footprint is minimal / microservice). HOT will use a single centralized Hanko instance, while self-hosters will primarily have a bundled Hanko instance per tool (it's unlikely they will be deploying all of our tools together).</p>"},{"location":"decisions/0011-sso-auth/#consequences","title":"Consequences","text":"<ul> <li>\u2705 Reduced developer overhead: code and key management.</li> <li>\u2705 Better user experience, with only a single login required.</li> <li>\u2705 Easy configuration of any OAuth provider we want underneath,   rolled out to all apps at the same time. Self-hosters can also   use any combination of providers they wish.</li> <li>\u2705 We rely on standard OIDC flows: the frontend does PKCE with   the IdP, gets a JWT, stores it in an HttpOnly cookie, sends it to   the API, which verifies the JWT signature and claims.</li> <li>\u274c Adds a single point of failure for auth (mitigated by running in   kubernetes with multiple replicas).</li> <li>\u274c Increases complexity of deployment slightly. The HOT deployment   relies on the centralised <code>login.hotosm.org</code> provider. For self-hosters,   we will have to provide a config that bundles the small Hanko binary   to host their own IdP.</li> </ul>"},{"location":"dev-guide/accessibility/","title":"Accessibility","text":"<p>Accessibility in this context means making your app as easy as possible to use for users with impairments of any kind.</p> <p>In the context of web this would typically only involve visual and cognitive impairments, or low literacy levels.</p> <p>With this in mind, an application should:</p> <ul> <li>Have ARIA labels in the UI component (must be included in development).</li> <li>Have large print / contrast options (this can typically be covered by   any modern web browser).</li> <li>Using icons rather than text where possible.</li> <li>Making application usage as intuitive as possible.</li> </ul>"},{"location":"dev-guide/accessibility/#aria-component-design","title":"ARIA &amp; Component Design","text":"<p>The following sections will describe how to make a web UI as accessible as possible.</p> <p>W3 maintains a guide on ARIA Authoring Practices</p>"},{"location":"dev-guide/accessibility/#keyboard-interaction","title":"Keyboard Interaction","text":"<ul> <li>The component can be selected using a keyboard button (e.g. tab key).</li> <li>Add <code>tabindex='0'</code> to your control or possibly <code>aria-activedescendant</code>.</li> </ul>"},{"location":"dev-guide/accessibility/#visually-intuitive-states","title":"Visually Intuitive States","text":"<ul> <li>When a component changes state, e.g. <code>focus</code>, <code>hover</code>, <code>active</code>, the UI   change should be clearly defined by a distinctive styling.</li> </ul>"},{"location":"dev-guide/accessibility/#aria-labels-for-state","title":"ARIA Labels For State","text":"<ul> <li>When a component changes state, there are often ARIA labels to help   describe this.</li> <li><code>aria-expanded</code> can be used when a modal is toggled open or closed.</li> <li>A multiline textbox can have <code>aria-multiline</code> to better communicate this.</li> <li><code>aria-selected</code> can be attached to each item in a dropdown to indicate   selected state.</li> <li>Full W3 guide</li> </ul>"},{"location":"dev-guide/accessibility/#aria-labels-for-function","title":"ARIA Labels For Function","text":"<ul> <li>A component can be labeled with it's stated function.</li> <li>For example a button may save your work, so should be labelled <code>Save</code>.</li> <li>Sometimes the labelled is implicit, for example if the button is defined   as such <code>&lt;button&gt;Save&lt;/button&gt;</code>, then the text content will be used.</li> <li><code>aria-label</code> is used in all other cases, or to override the label.   <code>&lt;button aria-label=\"Save\"&gt;Save The Thing&lt;/button&gt;</code></li> <li> <p><code>aria-labelledby</code> is used when we want to refer to another component   for labelling:</p> <pre><code>&lt;input type=\"search\" aria-labelledby=\"this\" /&gt;\n&lt;button id=\"this\"&gt;Search&lt;/button&gt;\n</code></pre> </li> </ul> <p>This is far from an exhaustive list on available accessibility options!</p>"},{"location":"dev-guide/api-doc-gen/","title":"Auto Documentation Generation","text":""},{"location":"dev-guide/api-doc-gen/#python","title":"Python","text":"<ul> <li>Ensure that functions and class method have descriptive docstrings.</li> <li>Type hinting should be used for parameter and return types.</li> <li>With this in place, there are many tools to automatically extract   API documentation from your code.<ul> <li>MKDocs has a great extension <code>mkdocstrings-python</code> for this purpose.</li> <li>Example markdown page to automatically extract docs:</li> </ul> </li> </ul> <pre><code># your_module.py\n\n::: your_package.your_module.function1\noptions:\nshow_source: false\nheading_level: 3\n\n::: your_package.your_module.function2\noptions:\nshow_source: false\nheading_level: 3\n\n::: your_package.your_module.Class1\noptions:\nshow_source: false\nheading_level: 3\n</code></pre> <p>Note that all methods within a class a picked up automatically.</p>"},{"location":"dev-guide/api-doc-gen/#javascript","title":"Javascript","text":"<ul> <li>JSDoc docstrings are the standard here.</li> <li>When adding typing (typescript) to your project, there are two options:<ul> <li>TypeScript: code-based, compiled TS --&gt; JS.</li> <li>JSDoc type hints: docstring-based, built on standard JS.</li> </ul> </li> </ul>"},{"location":"dev-guide/intro/","title":"Development Guide","text":"<p>The HOT Tech Team comprises a dynamic assembly of highly skilled individuals, each bringing a diverse range of expertise and backgrounds. United by a shared commitment, our goal is clear: leveraging technology to contribute to a better world.</p> <p>The tech team includes a core of software developers who co-develop many of the tools listed in this documentation index.</p> <p>There is also a large community of developers who helps to facilitate this work: from consultants, NGOs, and innovative tech agencies distributed throughout the world.</p> <p>The aim of this development guide is twofold:</p>"},{"location":"dev-guide/intro/#guidelines-for-collaborators","title":"Guidelines for Collaborators","text":"<p>Open-source is at the heart of everything we do. In order to collaborate effectively, it is important to have a set of agreed upon standards and practices that developers must loosely follow.</p>"},{"location":"dev-guide/intro/#a-resource-for-new-developers","title":"A Resource for New Developers","text":"<p>We encourage new developers to start their open-source contribution journey with us. We are a welcoming, considerate, and patient bunch.</p> <p>The consensus-based information here is not an authoritative source of the only approach to software development. Nevertheless, we firmly believe that adhering to the recommendations in this guide will undoubtedly set you on the path to becoming a skilled software professional, grounded in our collective experiences at HOT.</p> <p>If these documents help a single new developer on their journey from coding zero to hero, then it has been a success.</p>"},{"location":"dev-guide/testing/","title":"Testing","text":""},{"location":"dev-guide/testing/#types-of-testing","title":"Types of Testing","text":""},{"location":"dev-guide/testing/#unit-testing","title":"Unit Testing","text":"<ul> <li>Testing specific isolated functions or classes in your code.</li> <li>Inputs are pre-defined and outputs are mocked to verify the code   works as intended.</li> <li>Unit tests can be written for the backend (e.g. Pytest) and frontend   (e.g. Vitest).</li> </ul>"},{"location":"dev-guide/testing/#integration-testing","title":"Integration Testing","text":"<ul> <li>Verifies that different units or modules work together as expected.</li> <li>For example and endpoint on a web API can be tested as such:<ul> <li>Call and endpoint that triggers code in various modules.</li> <li>Standarise the input to the test, for example a specific JSON.</li> <li>Check the output data matches the expected output, every time it runs.</li> </ul> </li> <li>Typically backend tests, but can also be written to group together   multiple frontend unit tests (for example, testing a component state).</li> </ul>"},{"location":"dev-guide/testing/#end-to-end-e2e-testing","title":"End-to-End (E2E) Testing","text":"<ul> <li>Similar to integration tests, except the entire expected user   workflow is tested (i.e. the ordering is important).</li> <li>For example, if you expect a user to create a user account --&gt;   create a project --&gt; perform some processing, then these steps will   be tested in order (as if the user was using the software).</li> </ul>"},{"location":"dev-guide/testing/#user-interface-ui-testing","title":"User Interface (UI) Testing","text":"<ul> <li>Checking for changes to the user interface functionality can be   automated.</li> <li>As of 2024, a great tool to do this is   Playwright.</li> <li>The idea is to navigate to various pages and see if the output is as   expected.</li> <li>For example, a screenshot of a page can be taken on a page load, then   compared against a page load in a future test. This will determine if   any unexpected visual regression has occured.</li> <li>Page elements can be navigated and interacted with using various   properties, such as element   names, ids, labels, to lower level CSS and XPath if required.</li> </ul>"},{"location":"dev-guide/testing/#performance-testing","title":"Performance Testing","text":"<ul> <li>Evaluates the system's responsiveness, scalability, and stability   under different conditions.</li> <li>Includes:<ul> <li>Load testing, for expected and peak load.</li> <li>Stress testing, for abnormally high load / upper limit testing.</li> <li>Scalability testing, used to identify optimal configuration to handle load.</li> </ul> </li> <li>Generally for backend or database code.</li> <li>An example could be using a profiler to determine response time in a web API,   or using <code>EXPLAIN ANALYSE</code> on a database to determine bottlenecks.</li> </ul>"},{"location":"dev-guide/testing/#security-testing","title":"Security Testing","text":"<ul> <li>Identifies Common Vulnerabilities and Exposures (CVEs) in software.</li> <li>CVEs are publically available lists of identified security flaws in code.</li> <li>One way to test this could be container image scanning, which checks for   vulnerabilities in the underlying operating system, plus the code and dependencies.</li> </ul>"},{"location":"dev-guide/testing/#smoke-testing","title":"Smoke Testing","text":"<ul> <li>A test to pretty much see if your application starts up.</li> <li>If a smoke test fails, then you application failed to initialise.</li> <li>Particularly useful to test if a container runs as expected.</li> <li>Useful to include in CI to prevent deployment if smoke test fails.</li> </ul>"},{"location":"dev-guide/testing/#testing-terminology","title":"Testing Terminology","text":""},{"location":"dev-guide/testing/#white-box-testing","title":"White Box Testing","text":"<ul> <li>Examines the internal logic, code structure, and implementation   details of the software.</li> <li>Requires knowledge of the internal workings of the application.</li> </ul>"},{"location":"dev-guide/testing/#black-box-testing","title":"Black Box Testing","text":"<ul> <li>Tests the software's functionality without knowing its internal code or logic.</li> <li>Focuses on inputs and outputs, treating the software as a \"black box\".</li> </ul>"},{"location":"dev-guide/testing/#alpha-testing","title":"Alpha Testing","text":"<ul> <li>Conducted by a select group of users before the software's public release.</li> <li>Focuses on identifying major issues before wider deployment.</li> </ul>"},{"location":"dev-guide/testing/#beta-testing","title":"Beta Testing","text":"<ul> <li>Conducted by a larger group of users in a real-world environment.</li> <li>Gathers feedback from end-users to improve the software before the final release.</li> </ul>"},{"location":"dev-guide/testing/#python","title":"Python","text":""},{"location":"dev-guide/testing/#pytest","title":"PyTest","text":"<ul> <li>PyTest is a popular unit testing framework for Python, with more   in-built functionality than the standard Python <code>unittest</code> module.</li> <li>It has a simple and concise syntax, many plugins, plus async support,   and detailed / informative reporting.</li> <li>Two major advantages are:<ul> <li>Fixtures: allows you to set up and tear down resources during   testing, for example database entries.</li> <li>Parameterised testing: allowing you to run the same test with   different input values.</li> </ul> </li> <li>With the help of plugins, it's easy to write integration tests too.</li> <li>For example a FastAPI endpoint can be called with inputs, to test the   outputs are as expected.</li> </ul> <p>Usage:</p> <pre><code>pytest\n</code></pre>"},{"location":"dev-guide/testing/#coverage","title":"Coverage","text":"<ul> <li>The concept of coverage is to determine the amount of your code that   is executed during unit tests.</li> <li>This metric is a rough approximation of how thoroughly your tests   actually test your code.</li> <li>Use with care, this is only an assessment of how much code is touched by   your tests. A 100% coverage codebase may still be poorly tested.</li> </ul> <p>Usage:</p> <pre><code>coverage run -m pytest\n</code></pre> <p>Generating reports:</p> <pre><code>coverage report -m\n</code></pre>"},{"location":"dev-guide/testing/#javascript","title":"Javascript","text":""},{"location":"dev-guide/testing/#vitest","title":"ViTest","text":""},{"location":"dev-guide/testing/#playwright","title":"Playwright","text":""},{"location":"dev-guide/web-backend/","title":"Web APIs","text":""},{"location":"dev-guide/web-backend/#types","title":"Types","text":"<p>As a small aside, REST is not the only standard available when it comes to web APIs.</p>"},{"location":"dev-guide/web-backend/#rest","title":"REST","text":"<p>REST has dominated the scene for quite a few years.</p> <p>URLs are mapped to different HTTP methods (GET, POST, PUT, DELETE) to perform an action when called.</p> <p>Responses can be divided between Data APIs (return JSON) vs Hypermedia APIs (return HTML).</p>"},{"location":"dev-guide/web-backend/#graphql","title":"GraphQL","text":"<p>Without going into the details, this standard has many advantages over REST Data APIs, with much more efficient queries being possible.</p>"},{"location":"dev-guide/web-backend/#rpc","title":"RPC","text":"<p>The Remote Procedure Call (RPC) protocol can return XML or JSON responses.</p> <p>It is used to trigger code remotely, so good for internal communication between different services. HOT uses gRPC for internal communication between tools, outside of their REST API.</p> <p>While a RESTful API returns a document, the response from an RPC server is confirmation that the function was triggered, or an error indicating why it failed to run.</p>"},{"location":"dev-guide/web-backend/#others","title":"Others","text":"<p>SOAP is a historic API design using XML, and is no longer recommended.</p>"},{"location":"dev-guide/web-backend/#what-to-choose","title":"What To Choose","text":"<p>As of 2023, Data APIs have been key for the adoption of Single Page Applications (SPA) and Javascript frameworks (where JSON data is manipulated by the frontend).</p> <p>Going forward, Hypermedia APIs are re-emerging as an increasingly important alternative, where the entire page is rendered before being returned (reducing the need for things like Server Side Rendering (SSR)).</p> <p>The HTMX website has many interesting essays on this topic.</p> <p>In summary, it is probably best to default to a Hypermedia REST API, with a simple web framework like HTMX. If a much more complex frontend is required (such as a word processor, graphics editor, complex map), then a Data REST API is the best option.</p>"},{"location":"dev-guide/web-backend/#frameworks","title":"Frameworks","text":"<p>API Frameworks are generally divided into synchronous and asynchronous.</p> <p>Async is a newer paradigm in Python, often slightly more complex to code, but should be faster and more suited to a web API.</p> <p>Synchronous frameworks include flask, Django, etc.</p> <p>The asynchronous framework we recommend at HOT, as of 2024, is FastAPI. It's what we use for most projects.</p> <p>There is a great comparison with other frameworks in the ecosystem available.</p> <p>Another contender would be LiteStar, a project spawned from some frustrations with the governance of FastAPI.</p>"},{"location":"dev-guide/web-backend/#fastapi","title":"FastAPI","text":"<p>These docs provide some helpful info for FastAPI best practices.</p>"},{"location":"dev-guide/web-backend/#async-programming","title":"Async Programming","text":"<p>Asynchronous programming can be a learning curve for Python developers.</p> <ul> <li>FastAPI is an asynchronous web framework that is built to use async code.</li> <li>Using async (<code>async def</code>) function with await is more scalable than   using synchronous code <code>def</code>, so this is always the preferred default   approach.</li> <li>Using synchronous code is possible, but devs should be aware of the pitfalls:   if the code runs for a long time, it will block the async event loop   (i.e. block the thread until the process completes).<ul> <li>Bear in mind that 'synchronous' code could be from what you write   in the crud functions, OR could be from a library that you use   (e.g. osm-fieldwork is synchronous for the most part).</li> </ul> </li> </ul>"},{"location":"dev-guide/web-backend/#workers-processes-threads","title":"Workers, Processes, Threads","text":"<ul> <li>FastAPI uses <code>uvicorn</code> as the actual web server.</li> <li>Uvicorn has a number of <code>workers</code> defined. One worker equals   one process running on the system (using multiprocessing underneath).<ul> <li>Each process is independent and does not share memory.</li> <li>Each process is essentially a separate Python interpreter.</li> </ul> </li> <li>Each worker process will have it's own <code>event loop</code> for running async   code:<ul> <li>The event loop runs in the main thread of the worker process.</li> <li>This main thread handles all incoming requests, unless offloaded   to a different thread or process.</li> </ul> </li> <li>If the main thread blocks (e.g., via a sync function doing heavy I/O or   CPU work), then the API server inside that worker becomes unresponsive!</li> <li>To mitigate this, we must either:<ul> <li>Use a ThreadPoolExecutor (run_in_threadpool()): runs blocking tasks   in a separate thread inside the same worker process.</li> <li>Use a ProcessPoolExecutor: spawns a new process outside the worker   process to handle CPU-heavy tasks.</li> <li>Use BackgroundTasks (FastAPI feature): Runs tasks after returning   the response, often in a separate thread inside the same worker process.</li> </ul> </li> </ul> <p>Workers / processes --&gt; event loop --&gt; threads.</p>"},{"location":"dev-guide/web-backend/#using-synchronous-code","title":"Using Synchronous Code","text":"<p>It is of course possible to use synchronous code, but if necessary, be sure to run this in another thread.</p> <p>To do this you have several options.</p>"},{"location":"dev-guide/web-backend/#options","title":"Options","text":""},{"location":"dev-guide/web-backend/#1-using-sync-code-within-an-async-def-function","title":"1) Using sync code within an <code>async def</code> function","text":"<ul> <li>Use FastAPI BackgroundTasks, with polling for the task completion.<ul> <li>The task should be written as a standard <code>def</code>. FastAPI will handle   this automatically and ensure it runs in a separate thread.</li> </ul> </li> <li>Alternatively, if you wish to run the task in the foreground and return   the response, use the FastAPI helper <code>run_in_threadpool</code>.<ul> <li>This will run the function in a separate thread to ensure that the main   thread does not get blocked.</li> </ul> </li> </ul> <pre><code>from fastapi.concurrency import run_in_threadpool\n\ndef long_running_sync_task(time_to_sleep):\n    sleep(time_to_sleep)\n\nasync def some_func():\n    data = await run_in_threadpool(lambda: long_running_sync_task(10))\n</code></pre>"},{"location":"dev-guide/web-backend/#2-running-multiple-standard-def-from-within-an-async-def-function","title":"2) Running multiple standard <code>def</code> from within an <code>async def</code> function","text":"<ul> <li>Sometimes you need to run multiple <code>def</code> functions in parallel.</li> <li>To do this, you can use ThreadPoolExecutor:</li> </ul> <pre><code>from concurrent.futures import ThreadPoolExecutor, wait\n\ndef a_synchronous_function(db):\n    # Run with expensive task via threadpool\n    def wrap_generate_task_files(task):\n        \"\"\"Func to wrap and return errors from thread.\n\n        Also passes it's own database session for thread safety.\n        If we pass a single db session to multiple threads,\n        there may be inconsistencies or errors.\n        \"\"\"\n        try:\n            generate_task_files(\n                next(get_db()),\n                project_id,\n                task,\n                xlsform,\n                form_type,\n                odk_credentials,\n            )\n        except Exception as e:\n            log.exception(str(e))\n\n    # Use a ThreadPoolExecutor to run the synchronous code in threads\n    with ThreadPoolExecutor() as executor:\n        # Submit tasks to the thread pool\n        futures = [\n            executor.submit(wrap_generate_task_files, task)\n            for task in tasks_list\n        ]\n        # Wait for all tasks to complete\n        wait(futures)\n</code></pre> <p>Note that in the above example, we cannot pass the db object from the parent function into the functions spawned in threads. This is becaue a single database connection should not be written to by multiple processes at the same time, as you may get data inconsistencies. To solve this we generate a new db connection within the pool for each separate task we run in a thread.</p> <p>To avoid issues, look into limiting the thread usage via: https://stackoverflow.com/questions/73195338/how-to-avoid-database-connection-pool-from-being-exhausted-when-using-fastapi-in</p>"},{"location":"dev-guide/web-backend/#3-running-an-async-def-within-a-sync-def","title":"3) Running an <code>async def</code> within a sync <code>def</code>","text":"<ul> <li>As we try to write most functions async for FastAPI, sometime we need to   run some <code>async def</code> logic within a sync <code>def</code>. This is not possible normally.</li> <li>To avoid having to write a duplicated <code>def</code> equivalent of the <code>async def</code>   code, we can use the package <code>asgiref</code>:</li> </ul> <pre><code>from asgiref.sync import async_to_sync\n\nasync def get_project(db, project_id):\n    return something\n\ndef a_sync_function():\n     get_project_sync = async_to_sync(get_project)\n     project = get_project_sync(db, project_id)\n     return project\n</code></pre>"},{"location":"dev-guide/web-backend/#4-efficiency-running-batch-async-tasks","title":"4) Efficiency running batch async tasks","text":"<ul> <li>Sometime you may have a very efficient async task you need to call   within a for loop.</li> <li>Instead of that, you can use <code>asyncio.gather</code> to much more efficiently   collect and return the async data (e.g. async web requests, or async   file requests, or async db requests):</li> </ul> <pre><code>from asyncio import gather\n\nasync def parent_func(db, project_id, data, no_of_buildings, has_data_extracts):\n    ... some other code\n\n    async def split_multi_geom_into_tasks():\n        # Use asyncio.gather to concurrently process the async generator\n        split_poly = [\n            split_polygon_into_tasks(\n                db, project_id, data, no_of_buildings, has_data_extracts\n            )\n            for data in boundary_geoms\n        ]\n\n        # Use asyncio.gather with list to collect results from the\n        # async generator\n        return (\n            item for sublist in await gather(*split_poly)\n            for item in sublist if sublist\n        )\n\n    geoms = await split_multi_geom_into_tasks()\n</code></pre>"},{"location":"dev-guide/web-backend/#5-running-computationally-intensive-tasks","title":"5) Running computationally intensive tasks","text":"<ul> <li>Most code on a web server is typically IO-bound, meaning a related to   file operations, or a web request.</li> <li>But sometimes we must run code that demands high CPU usage over time.</li> <li>In these cases, it is best to use <code>concurrent.futures.ProcessPoolExecutor</code>   built into Python.</li> </ul> <pre><code>from asyncio import get_running_loop\nfrom concurrent.futures import ProcessPoolExecutor\n\nasync def run_generate_project_basemap():\n    loop = get_running_loop()\n    with ProcessPoolExecutor() as pool:\n        return await loop.run_in_executor(\n            pool,  # process pool\n            project_crud.generate_project_basemap,  # function\n            db_pool,  # args\n            project_id,\n            org_id,\n            background_task_id,\n            basemap_in.tile_source,\n            basemap_in.file_format,\n            basemap_in.tms_url,\n        )\n</code></pre> <p>Note this is very similar to MultiProcessing in Python, but the code is much simpler to use (requires less manual config).</p>"},{"location":"dev-guide/web-backend/#note","title":"Note","text":"<ul> <li>If you regularly find you are running out of workers/threads and the   server is overloaded, it may be time to add a task queuing system to your stack.</li> <li>Celery is made for just this - defer tasks to a queue, and run gradually   to reduce the immediate load.</li> </ul>"},{"location":"dev-guide/web-backend/#best-practices-tips","title":"Best Practices / Tips","text":""},{"location":"dev-guide/web-backend/#1-logical-project-structure","title":"1. Logical Project Structure","text":"<ul> <li>Group together related code into units.</li> <li>An example template could be:</li> </ul> <pre><code>fastapi-project\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 projects\n\u2502   \u2502   \u251c\u2500\u2500 routes.py  # endpoints + router\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py  # pydantic models\n\u2502   \u2502   \u2514\u2500\u2500 logic.py  # logic separate from routes for easier testing\n\u2502   \u251c\u2500\u2500 tasks\n\u2502   \u2502   \u251c\u2500\u2500 routes.py\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py\n\u2502   \u2502   \u2514\u2500\u2500 logic.py\n\u2502   \u251c\u2500\u2500 db\n\u2502   \u2502   \u251c\u2500\u2500 models.py  # global database models (can also be per subdir)\n\u2502   \u2502   \u251c\u2500\u2500 enums.py  # enum mapping for the database\n\u2502   \u2502   \u2514\u2500\u2500 database.py  # database connection config\n\u2502   \u251c\u2500\u2500 config.py  # global settings\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 tests/\n</code></pre>"},{"location":"dev-guide/web-backend/#2-use-the-correct-response-type","title":"2. Use the Correct Response Type","text":"<ul> <li>FastAPI has many in-built Response types:<ul> <li>HTMLResponse: this would be useful paired with a HTMX frontend.</li> <li>JSONResponse: to return a JSON.</li> <li>ORJSONResponse: a faster JSON encoder. If you need to encode a large number   of object, this might be a good choice.</li> <li>RedirectResponse: use this for linking to an S3 file, to avoid handling it   in the FastAPI server (frontend goes directly to S3).</li> <li>FileResponse: load an entire file from disk and serve to the user.</li> <li>StreamingResponse: better for serving large file in chunks.</li> </ul> </li> </ul> <ul> <li>Don't forget to include the correct HTTP <code>status_code</code> with your response:<ul> <li>200: Success, used as the final return for most endpoints.</li> <li>204: Success, but no response data necessary.</li> <li>400: Bad request, usually malformed syntax or incorrect HTTP method (POST/GET).</li> <li>401: Unauthorized, if the client does did not provide an auth token.</li> <li>403: Forbidden, if the client does not have permission to access the content.</li> <li>404: Not found, if the requested content is not present. E.g. wrong project id.</li> <li>422: Unprocessable entity, if the request data is in the incorrect format.   e.g. a string provided in a form body variable when it should be an int.</li> <li>500: Generic error if no other error is provided, like Exception in Python.</li> </ul> </li> </ul>"},{"location":"dev-guide/web-backend/#3-use-pydantic-for-validation","title":"3. Use Pydantic for Validation","text":""},{"location":"dev-guide/web-backend/#settings-config","title":"Settings Config","text":"<pre><code>from functools import lru_cache\nfrom typing import Any, Optional\n\nfrom pydantic import PostgresDsn, ValidationInfo, field_validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    \"\"\"Main settings class, defining environment variables.\"\"\"\n\n    # Required field\n    VAR1: str\n    # Required field, but nullable\n    VAR2: Optional[str]\n    # Required field, with default\n    VAR3: Optional[str] = \"7050\"\n    # Not required field\n    VAR4: Optional[str] = None\n\n    DB_HOST: Optional[str] = \"fmtm-db\"\n    DB_USER: Optional[str] = \"fmtm\"\n    DB_PASSWORD: Optional[str] = \"fmtm\"\n    DB_NAME: Optional[str] = \"fmtm\"\n\n    FMTM_DB_URL: Optional[PostgresDsn] = None\n\n    # Using a field validator to build a variable\n    @field_validator(\"FMTM_DB_URL\", mode=\"after\")\n    @classmethod\n    def assemble_db_connection(cls, v: Optional[str], info: ValidationInfo) -&gt; Any:\n        \"\"\"Build Postgres connection from environment variables.\"\"\"\n        if isinstance(v, str):\n            return v\n        pg_url = PostgresDsn.build(\n            scheme=\"postgresql\",\n            username=info.data.get(\"DB_USER\"),\n            password=info.data.get(\"DB_PASSWORD\"),\n            host=info.data.get(\"DB_HOST\"),\n            path=info.data.get(\"DB_NAME\", \"\"),\n        )\n        return pg_url\n\n    # Using env_file param loads from .env\n    model_config = SettingsConfigDict(\n        case_sensitive=True, env_file=\".env\", extra=\"allow\"\n    )\n\n# lru_cache prevents building obj every time settings.var is invoked\n@lru_cache\ndef get_settings():\n    \"\"\"Cache settings when accessed throughout app.\"\"\"\n    _settings = Settings()\n    if _settings.DEBUG:\n        print(f\"Loaded settings: {_settings.model_dump()}\")\n    return _settings\n\nsettings = get_settings()\n</code></pre>"},{"location":"dev-guide/web-backend/#model-validation","title":"Model Validation","text":"<ul> <li>Used for 'incoming' (user provided) data that needs to be validated.</li> </ul> <pre><code>from enum import Enum\nfrom pydantic import AnyUrl, BaseModel, EmailStr, Field, constr\n\nclass MusicBand(str, Enum):\n   AEROSMITH = \"AEROSMITH\"\n   QUEEN = \"QUEEN\"\n   ACDC = \"AC/DC\"\n\n\nclass UserBase(BaseModel):\n    first_name: str = Field(min_length=1, max_length=128)\n    username: constr(regex=\"^[A-Za-z0-9-_]+$\", to_lower=True, strip_whitespace=True)\n    email: EmailStr\n    age: int = Field(ge=18, default=None)  # must be greater or equal to 18\n    # only \"AEROSMITH\", \"QUEEN\", \"AC/DC\" values are allowed to be inputted\n    favorite_band: MusicBand = None\n    website: AnyUrl = None\n    valid_genre: Optional[boolean] = False\n\n    @field_validator(\"valid_genre\", mode=\"before\")\n    @classmethod\n    def get_genre_from_band_name(cls, value: Any, info: ValidationInfo) -&gt; str:\n        \"\"\"Get genre from band name.\"\"\"\n        if band := info.data.get(\"favorite_band\"):\n            log.debug(f\"Determining genre from band {band}\")\n            genre = band_genre_mapping(band)\n            if genre:\n                return True\n        return False\n</code></pre>"},{"location":"dev-guide/web-backend/#model-data-serialization","title":"Model Data Serialization","text":"<ul> <li>Used to format 'outgoing' data that is returned to a user.</li> </ul> <pre><code>class TaskBase(BaseModel):\n    \"\"\"Base Task model to inherit.\"\"\"\n    # ConfigDict has many options\n    # https://docs.pydantic.dev/latest/api/config/\n    # E.g. use_enum_values automatically runs .value on enums\n    # So a returned object will have\n    #   `somefield: 1`\n    # instead of\n    #   `somefield: SomeEnum.TYPE1`\n    model_config = ConfigDict(\n        use_enum_values=True,\n        validate_default=True,\n    )\n\n    # Exclude fields: for example we want to get these values from the database,\n    # and then process them into different fields in our returned model.\n    # outline (a WKB element from Postgis) --&gt; outline_geojson\n    outline: Any = Field(exclude=True)\n    lock_holder: Any = Field(exclude=True)\n\n    id: int\n    outline_geojson: Optional[Feature] = None\n    task_history: Optional[List[TaskHistoryBase]] = None\n\n\nclass TaskOut(TaskBase):\n    \"\"\"Task to return from endpoint.\"\"\"\n\n    locked_by_uid: Optional[int] = None\n    outline_geojson: Optional[int] = None\n\n    @field_serializer(\"locked_by_uid\")\n    def get_locked_by_uid(self, value: str) -&gt; str:\n        \"\"\"Get lock uid from lock_holder details.\"\"\"\n        if self.lock_holder:\n            return self.lock_holder.id\n        return None\n\n    @field_serializer(\"outline_geojson\")\n    def get_geojson_from_outline(self, value: Any, info: ValidationInfo) -&gt; str:\n        \"\"\"Get outline_geojson from Shapely geom.\"\"\"\n        if outline := info.data.get(\"outline\"):\n            properties = {\n                \"fid\": info.data.get(\"project_task_index\"),\n                \"uid\": info.data.get(\"id\"),\n                \"name\": info.data.get(\"project_task_name\"),\n            }\n            log.debug(\"Converting task outline to geojson\")\n            return geometry_to_geojson(outline, properties, info.data.get(\"id\"))\n        return None\n</code></pre>"},{"location":"dev-guide/web-backend/#response-models","title":"Response models","text":"<ul> <li>FastAPI integrates Pydantic very nicely.</li> <li>Endpoints allow us to define a <code>response_model</code>, which is a Pydantic model.</li> <li>This specifies the fields that must be present in the endpoint JSON response.</li> <li>Validators and serialisers are all called when a response_model is used.<ul> <li>This means that formatting and validation of the returned data does not   need to be done in the endpoint code.</li> <li>It is instead handled by Pydantic, and will throw an error if validation   does not pass.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># project_schemas.py\nclass ProjectBase(BaseModel):\n    id: int\n    name: str\n\nclass ProjectInt(ProjectBase)\n    organization: str  # org abbreviation provided by frontend\n\n    @field_validator(\"organization\", mode=\"before\")\n    @classmethod\n    def get_org_long_name(cls, value: str) -&gt; str:\n        return get_org_long_name_from_abbreviation(value)\n\nclass ProjectOut(ProjectBase):\n    date_created: datetime.date\n\n    @field_serializer(\"date_created\")\n    def format_date(self, value: datetime.date):\n          # Format: Monday 01 2023\n          return last_active.strftime(\"%d %b %Y\")\n\n\n# project_routes.py\n@router.put(\"/{id}\", response_model=ProjectOut)\nasync def update_project(\n    id: int,\n    project_info: ProjectIn,\n    db: Session = Depends(database.get_db),\n):\n    \"\"\"Update an existing project by ID.\"\"\"\n    project = await project_crud.update_project_info(db, project_info, id)\n    if not project:\n        raise HTTPException(status_code=422, detail=\"Project update failed\")\n    return project\n</code></pre>"},{"location":"dev-guide/web-backend/#4-fastapi-dependencies-depends","title":"4. FastAPI Dependencies (Depends)","text":""},{"location":"dev-guide/web-backend/#validation-of-additional-constraints","title":"Validation of additional constraints","text":"<ul> <li>Pydantic can only validate the 'incoming' data from client input.</li> <li>Use dependencies (Depends) to validate input against other constraints:<ul> <li>Database constraints, such as project or email already exists, user not found.</li> <li>Auth constraints, where the users level of authorization should be assessed   in an endpoint.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># logic.py (where the dependency is written)\nasync def valid_post_id(post_id: UUID4) -&gt; Mapping:\n    post = await service.get_by_id(post_id)\n    if not post:\n        raise PostNotFound()\n\n    return post\n\n\n# routes.py (where Depends is used)\n@router.get(\"/posts/{post_id}\", response_model=PostResponse)\nasync def get_post_by_id(post: Mapping = Depends(valid_post_id)):\n    return post\n\n\n@router.put(\"/posts/{post_id}\", response_model=PostResponse)\nasync def update_post(\n    update_data: PostUpdate,\n    post: Mapping = Depends(valid_post_id),\n):\n    updated_post: Mapping = await service.update(id=post[\"id\"], data=update_data)\n    return updated_post\n</code></pre> <p>If we didn't put data validation in a dependency, we would have to do the same checks for on each endpoint (duplicating code).</p>"},{"location":"dev-guide/web-backend/#reuse-chain-dependencies","title":"Reuse &amp; chain dependencies","text":"<ul> <li>Dependencies can use other dependencies and repeating code.</li> </ul> <p>Example:</p> <pre><code># logic.py\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\n\n# Depends on pre-existing FastAPI dependency OAuth2PasswordBearer\nasync def parse_jwt_data(\n    token: str = Depends(OAuth2PasswordBearer(tokenUrl=\"/auth/token\"))\n) -&gt; dict:\n    try:\n        payload = jwt.decode(token, \"JWT_SECRET\", algorithms=[\"HS256\"])\n    except JWTError:\n        raise InvalidCredentials()\n\n    return {\"user_id\": payload[\"id\"]}\n\n# Depends on parse_jwt_data (chained)\nasync def valid_owned_post(\n    post: Mapping = Depends(valid_post_id),\n    token_data: dict = Depends(parse_jwt_data),\n) -&gt; Mapping:\n    if post[\"creator_id\"] != token_data[\"user_id\"]:\n        raise UserNotOwner()\n\n    return post\n\n# routes.py (where the final Depends is used)\n@router.get(\"/users/{user_id}/posts/{post_id}\", response_model=PostResponse)\nasync def get_user_post(post: Mapping = Depends(valid_owned_post)):\n    return post\n</code></pre>"},{"location":"dev-guide/web-backend/#dependency-call-are-cached","title":"Dependency call are cached","text":"<ul> <li>Dependencies can be reused multiple times, and they won't be recalculated.</li> <li>FastAPI caches dependency's result within a request's scope by default:<ul> <li>If a dependency makes a DB call, this can be cached when the dependency is   called again.</li> <li>With this in mind, try to de-couple dependencies, i.e. write smaller   functions that do specific things, then chain them.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># logic.py (contains dependencies here)\nfrom fastapi import BackgroundTasks\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\n\n\n# Dependency 1\nasync def valid_post_id(post_id: UUID4) -&gt; Mapping:\n    post = await service.get_by_id(post_id)\n    if not post:\n        raise PostNotFound()\n\n    return post\n\n# Dependency 2\nasync def parse_jwt_data(\n    token: str = Depends(OAuth2PasswordBearer(tokenUrl=\"/auth/token\"))\n) -&gt; dict:\n    try:\n        payload = jwt.decode(token, \"JWT_SECRET\", algorithms=[\"HS256\"])\n    except JWTError:\n        raise InvalidCredentials()\n\n    return {\"user_id\": payload[\"id\"]}\n\n# Dependency 3 uses both 1 &amp; 2\nasync def valid_owned_post(\n    post: Mapping = Depends(valid_post_id),\n    token_data: dict = Depends(parse_jwt_data),\n) -&gt; Mapping:\n    if post[\"creator_id\"] != token_data[\"user_id\"]:\n        raise UserNotOwner()\n\n    return post\n\n# Dependency 4 also uses dependency 2 (and is cached)\nasync def valid_active_creator(\n    token_data: dict = Depends(parse_jwt_data),\n):\n    user = await users_service.get_by_id(token_data[\"user_id\"])\n    if not user[\"is_active\"]:\n        raise UserIsBanned()\n\n    if not user[\"is_creator\"]:\n       raise UserNotCreator()\n\n    return user\n\n\n# routes.py (uses both Dependency 3 &amp; 4)\n@router.get(\"/users/{user_id}/posts/{post_id}\", response_model=PostResponse)\nasync def get_user_post(\n    worker: BackgroundTasks,\n    post: Mapping = Depends(valid_owned_post),\n    user: Mapping = Depends(valid_active_creator),\n):\n    \"\"\"Get post that belong the active user.\"\"\"\n    worker.add_task(notifications_service.send_email, user[\"id\"])\n    return post\n</code></pre>"},{"location":"dev-guide/web-backend/#dependencies-can-include-route-parameters","title":"Dependencies can include route parameters","text":"<ul> <li>Sometimes a dependency requires additional variables to run it's logic.</li> <li>As an example we can imagine an app that has users and projects:<ul> <li>To determine if a user has permission to access a project we need both:<ul> <li>The user id</li> <li>The project id</li> </ul> </li> <li>The user id could be determined via another dependency.</li> <li>However, the project id must be passed in by the user.</li> </ul> </li> </ul> <pre><code># logic.py (where dependencies are located)\n\nfrom app.auth.osm import AuthUser, login_required # imported dependency\n\nasync def validator(\n    project_id: int, # The route parameter\n    db: Session = Depends(get_db),\n    user_data: AuthUser = Depends(login_required), # from imported dependency\n) -&gt; AuthUser:\n    user_id = await get_uid(user_data)\n\n    match = (\n        db.query(DbUserRoles).filter_by(user_id=user_id, project_id=project_id).first()\n    )\n\n    if not match:\n        raise HTTPException(status_code=403, detail=\"User has no access to project\")\n\n    if match.role.value &lt; ProjectRole.VALIDATOR.value:\n        raise HTTPException(\n            status_code=403, detail=\"User is not a validator for this project\"\n        )\n\n    return user_data\n\n# routes.py (endpoints)\n@router.get(\"/get_validator/\")\nasync def validator(\n    db: Session = Depends(database.get_db),\n    user: AuthUser = Depends(validator),\n):\n    return user\n</code></pre> <p>When the user calls the <code>/get_validator</code> endpoint, they will need to provide the parameter <code>project_id</code>, as it is present in the <code>validator</code> sub dependency.</p>"},{"location":"dev-guide/web-backend/#5-always-use-typing","title":"5. Always Use Typing","text":"<ul> <li>FastAPI relies on Typing heavily for it's functionality.</li> <li>Typing also helps linting and IDE code completion.</li> <li>Pydantic models can be used as types.</li> <li>If endpoints often reference data in the same format, it's useful to have a model.</li> </ul> <p>For example an authenticated user model:</p> <pre><code>class AuthUser(BaseModel):\n    id: int\n    username: str\n    img_url: Optional[str]\n\n# Usage\nuser: AuthUser = get_auth_user()\n</code></pre> <p>For FastAPI routes, it's good to use the <code>Annotated</code> class:</p> <pre><code>@router.get(\"/me\", response_model=FMTMUser)\nasync def my_data(\n    db: Annotated[Connection, Depends(db_conn)],\n    current_user: Annotated[AuthUser, Depends(login_required)],\n    another_param: Annotated[Optional[str], None],\n):\n</code></pre> <p>It allows us to very effectively define params, including defaults, and dependency injection.</p>"},{"location":"dev-guide/web-backend/#6-use-rest-endpoint-naming","title":"6. Use REST Endpoint Naming","text":"<p>REST APIs are formatted as such:</p> <pre><code>GET /projects/:project_id\nGET /projects/:project_id/tasks/:task_id/submissions\nGET /users/:user_id\n</code></pre> <p>In summary:</p> <ul> <li><code>projects</code> is the noun in this example.</li> <li>Always use plural nouns: <code>projects/xxx</code> vs <code>project/xxx</code>.</li> <li>Never use verbs in endpoint: <code>projects/11/create</code><ul> <li>Instead use GET, POST, PUT, PATCH, DELETE methods.</li> </ul> </li> </ul> <p>It is also recommended to add a version, e.g. <code>/v2/projects</code>, to the API.</p> <p>However, if the project is small, this may not always be necessary.</p>"},{"location":"dev-guide/web-backend/#7-save-files-in-chunks","title":"7. Save Files in Chunks","text":"<ul> <li>If the API needs to receive a large file from a user, receive it in chunks:</li> </ul> <pre><code>import aiofiles\nfrom fastapi import UploadFile\n\nDEFAULT_CHUNK_SIZE = 1024 * 1024 * 50  # 50 megabytes\n\nasync def save_video(video_file: UploadFile):\n   async with aiofiles.open(\"/file/path/name.mp4\", \"wb\") as f:\n     while chunk := await video_file.read(DEFAULT_CHUNK_SIZE):\n         await f.write(chunk)\n</code></pre>"},{"location":"dev-guide/web-backend/#8-file-uploads-alongside-other-data","title":"8. File Uploads Alongside Other Data","text":"<ul> <li>FormData is the only request type that can include both data fields and   data files at the same time.</li> <li>In the following example we have an endpoint that accepts params <code>submission_xml</code>   (large text field), and <code>device_id</code> (small text field), plus a list of attached   <code>submission_attachments</code> files uploaded alongside the data:</li> </ul> <pre><code># First we create a dependency for file uploads - this could easily be generic\nfrom io import BytesIO\nfrom typing import Optional\nfrom fastapi import UploadFile\nasync def read_submission_uploads(\n    submission_files: Optional[list[UploadFile]] = None,\n) -&gt; Optional[dict[str, BytesIO]]:\n    \"\"\"Read all uploaded submission attachments for upload to ODK Central.\"\"\"\n    if not submission_files:\n        return None\n\n    file_data_dict = {\n        file.filename: BytesIO(await file.read()) for file in submission_files\n    }\n    return file_data_dict\n\n# Then we define our endpoint and variables\n# Imports here...\n@router.post(\"\", response_model=CentralSubmissionOut) # response model for serialisation\nasync def create_submission(\n    project_user: Annotated[ProjectUserDict, Depends(mapper)], # check auth\n    submission_xml: Annotated[str, Body(embed=True)], # embed=True puts var in FormData\n    device_id: Annotated[Optional[str], Body(embed=True)] = None,\n    submission_attachments: Annotated[\n        Optional[dict[str, BytesIO]], Depends(submission_deps.read_submission_uploads)\n    ] = None, # We use a dependency to get the uploaded file name and file data\n              # then wrap them in a dict {filename1: bytesio_data1, ...}\n):\n    # Say we want to write the file data somewhere\n    for file_name, file_data in submission_attachments.items():\n        temp_path = f\"/tmp/{file_name}\"\n        with open(temp_path, \"wb\") as temp_file:\n            temp_file.write(file_data.getvalue())\n</code></pre>"},{"location":"dev-guide/web-frontend/","title":"JavaScript Frameworks","text":"<p>One of the lead devs writing these docs made a presentation about this</p>"},{"location":"dev-guide/web-frontend/#recommendations-summary","title":"Recommendations Summary","text":"<ul> <li>Simple CRUD app: use HTMX.</li> <li>Reactive UI &amp; collaborative: local-first + compiler-based framework.<ul> <li>This is mostly where HOTs tools fall.</li> </ul> </li> <li>Web business or complex reactive app: SvelteKit or similar.</li> <li>Organisation-wide consistent UI: cross-framework web components.</li> </ul>"},{"location":"dev-guide/web-frontend/#local-first","title":"Local-First","text":"<ul> <li>Various resources related to local-first development.</li> <li>In summary, this is possibly the future of web development.</li> <li>Instead of managing state on the frontend, we host a database   entirely in the frontend using WASM.</li> <li>This database is synced to the centralised database server.</li> <li>Benefits:<ul> <li>Realtime Sync: enables real-time synchronization between client-side   WASM databases and server-side databases.</li> <li>Conflict Resolution: provides mechanisms for handling conflicts and   ensuring data consistency across distributed systems.</li> <li>Offline Support: allows applications to function offline by maintaining   local state and syncing changes once back online.</li> </ul> </li> </ul>"},{"location":"dev-guide/web-frontend/#backend-or-frontend","title":"Backend or Frontend?","text":"<p>There are two crucial questions you should ask before deciding on the approach to web development taken:</p> <ul> <li>Does your team skillset lie more in backend or frontend?</li> <li>Do you need to optimise for low resource usage on end-user devices,   or for low connectivity?</li> </ul> <p>Depends on the answer, you may reach for either of the options below.</p> <p>Hypermedia-based: backend-dev solution | optimise backend:</p> <ul> <li>Server-centric \u2013 minimise frontend.</li> <li>Costs more money to run (servers).</li> <li>Not as fast as possible local-first solutions.</li> <li>But as stated, could have benefits when serving to low-spec devices.</li> <li>Small bundle size \u2013 less code to transfer.</li> </ul> <p>Frontend (local-first, WASM): frontend-dev solution | optimise frontend:</p> <ul> <li>Client-centric \u2013 minimise backend.</li> <li>Cheaper to run.</li> <li>May require heavy resource usage on the client: low-spec devices may   struggle.</li> <li>Larger bundle size \u2013 more code to transfer.</li> </ul>"},{"location":"dev-guide/containers/containers-101/","title":"Containers Intro","text":""},{"location":"dev-guide/containers/containers-101/#why-use-containers","title":"Why Use Containers","text":""},{"location":"dev-guide/containers/containers-101/#the-traditional-setup","title":"The Traditional Setup","text":"<ul> <li>Host machine resources divided up into multiple virtual machines.</li> <li>Each virtual machine running an application.</li> <li>This has no ability to scale resources on the virtual machines:<ul> <li>At times of heavy load the machine resources are limited.</li> <li>At times of low load the resources are under-utilised.</li> </ul> </li> </ul>"},{"location":"dev-guide/containers/containers-101/#the-new-way","title":"The New Way","text":"<ul> <li>Containers remove the need for virtual machines, by providing the   isolation required between applications while retaining access to the   essential low level operating system components on the main machine.<ul> <li>Instead of multiple separate operating systems running, a single   operating system is required. Much more efficient.</li> </ul> </li> </ul>"},{"location":"dev-guide/containers/containers-101/#cool-what-can-i-use-them-for","title":"Cool. What can I use them for?","text":"<ul> <li>Managing dependencies: isolating your code and dependencies from the   host operating system allows you to have different versions of   software installed in different containers.</li> <li>Running software without an install: for example the utility <code>rclone</code>   can be used from it's container image (a pre-packaged environment,   with all required dependencies: docker.io/rclone/rclone), instead of   installing directly on your machine. Clean.</li> <li>Packaging your code for distribution or deployment: written a   backend in Django? It can be packaged up into an image and deployed   anywhere that has a container engine. Your laptop, local server,   AWS, Azure\u2026 you name it.</li> </ul> <p>Containers are now the de-facto way to distribute software. Software developers are required to know the basics of what they are, how to build an image, and how to use a container.</p>"},{"location":"dev-guide/containers/containers-101/#definitions","title":"Definitions","text":"<p>Note: 'Docker' is often used in place of the word 'Container', as this was the main project to popularise containers.</p> <ul> <li>Container (Docker Image): essentially a frozen state of an   operating system, including filesystem, built-in command line tools,   and your application code. This is built from a series of build   instructions, almost exactly how you would have deployed your   application onto a virtual machine.</li> <li>Container: a container image is used to create a running container.   When you run a container, you may wish to specify a network to attach   to, files to mount into the container, and other things.</li> <li>Volume: when a container is shut down, the filesystem is normally   lost - it is ephemeral. A volume allows you to keep data after   the containers lifecycle.</li> </ul>"},{"location":"dev-guide/containers/containers-101/#docker-vs-kubernetes-vs-other","title":"Docker vs Kubernetes vs Other","text":"<ul> <li>In the graphic above, Docker would be the Container Engine. It is   what actually executes the commands to run the container and keep it running.</li> <li>Docker runs on your local machine with single containers.   Docker Inc made a product called Docker Swarm, to allow for the   management of containers across a fleet of servers.   It essentially lost the battle to a Google-backed tool called Kubernetes.</li> <li>Kubernetes is a container orchestration tool and now the standard for   how businesses deploy their software, in a way that is resilient to   server crashes, code logic errors, etc.<ul> <li>If a process fails on one server, e.g. a Django API server, it   will automatically be replaced by an equivalent container on another server.</li> </ul> </li> </ul>"},{"location":"dev-guide/containers/containers-101/#show-me-the-code","title":"Show Me the Code","text":""},{"location":"dev-guide/containers/containers-101/#running-containers","title":"Running containers","text":"<p>https://docs.docker.com/engine/reference/commandline/run/</p> <ul> <li>Run a simple Ubuntu container, based off the Ubuntu Focal image:</li> </ul> <pre><code>docker run -it docker.io/ubuntu:focal bash\n</code></pre> <ul> <li>The <code>-it</code> flag is to tell docker to open an interactive (<code>i</code>) terminal   (<code>t</code>) for you to type commands into the container.</li> <li>The command after the image name is simply <code>bash</code>, which runs a bash   terminal (as opposed to a basic shell terminal: <code>sh</code>).</li> <li>The other mode to run containers is detached (<code>-d</code>), but for this you   need a process to run, instead of a terminal session, for example   <code>python /app/code/main.py</code>.</li> </ul>"},{"location":"dev-guide/containers/containers-101/#building-images","title":"Building Images","text":"<p>Two components are required here:</p> <ul> <li>A Dockerfile (Containerfile). This contains the commands, in order,   that install the dependencies from base image (e.g. Ubuntu), then add   your application code into the image.</li> <li>A build instruction. The command line instruction to build an image,   giving it a name etc:   https://docs.docker.com/engine/reference/commandline/build/</li> </ul> <p>An example Dockerfile:</p> <pre><code># Use a base image with pre-installed dependencies\nFROM node:18-alpine\n# Set the directory to run commands in\nWORKDIR /app\n# Copy your code into the container image\nCOPY . .\n# Install your node dependencies to run the app\nRUN yarn install --production\n# Command to execute at container start (i.e. run a server)\nCMD [\"node\", \"src/index.js\"]\n</code></pre>"},{"location":"dev-guide/containers/containers-101/#additional-references","title":"Additional References","text":"<p>Example build: https://docs.docker.com/get-started/02_our_app/</p> <p>Lots of good tutorials can be found online, search for: Dockerfile build example / tutorial.</p>"},{"location":"dev-guide/containers/containers-cheat-sheet/","title":"Containers Cheat Sheet","text":""},{"location":"dev-guide/containers/kubernetes-local-kind/","title":"Testing with Kubernetes Locally","text":"<p>We will use an official tool Kubernetes-In-Docker (KIND) to run Kubernetes via Docker locally.</p> <p>This is useful for testing Kubernetes configuration locally, and trialing software too.</p>"},{"location":"dev-guide/containers/kubernetes-local-kind/#install-tools","title":"Install Tools","text":""},{"location":"dev-guide/containers/kubernetes-local-kind/#kubectl","title":"Kubectl","text":"<p>Used to control Kubernetes clusters.</p> <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kubectl \\\nhttps://dl.k8s.io/release/\\\n$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/\\\n$(uname -s | tr '[:upper:]' '[:lower:]')/amd64/kubectl\n\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n\nrm -rf \"$temp_dir\"\n</code></pre>"},{"location":"dev-guide/containers/kubernetes-local-kind/#kind","title":"KIND","text":"<p>Used to run a Kubernetes cluster locally.</p> <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind \\\nhttps://kind.sigs.k8s.io/dl/v0.22.0/kind-$(uname)-amd64\n\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\nrm -rf \"$temp_dir\"\n</code></pre> Optional Config   Kubie: used to easily switch Kubernetes context (i.e. multiple clusters).  <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kubie \\\nhttps://github.com/sbstp/kubie/releases/download/v0.23.0/kubie-\\\n$(uname -s | tr '[:upper:]' '[:lower:]')-amd64\n\nchmod +x ./kubie\nsudo mv ./kubie /usr/local/bin/kubie\n\nrm -rf \"$temp_dir\"\n</code></pre>  Helm: used to install software into the cluster.  <pre><code>temp_dir=$(mktemp -d)\ncd \"${temp_dir}\"\n\n[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./helm.tar.gz \\\nhttps://get.helm.sh/helm-v3.14.3-$(uname -s | tr '[:upper:]' '[:lower:]')-amd64.tar.gz\n\ntar -xvzf helm.tar.gz\nsudo mv $(uname -s | tr '[:upper:]' '[:lower:]')-amd64/helm /usr/local/bin/helm\n\nrm -rf \"$temp_dir\"\n</code></pre>  BASH (bashrc) aliases  <pre><code>echo alias k='kubectl' &gt;&gt; ~/.bashrc\necho alias kcc='kubie ctx' &gt;&gt; ~/.bashrc\necho alias ns='kubie ns' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>  fish aliases  <pre><code>echo alias k='kubectl' &gt;&gt; ~/.config/fish/config.fish\necho alias kcc='kubie ctx' &gt;&gt; ~/.config/fish/config.fish\necho alias ns='kubie ns' &gt;&gt; ~/.config/fish/config.fish\nsource ~/.config/fish/config.fish\n</code></pre>"},{"location":"dev-guide/containers/kubernetes-local-kind/#create-a-cluster","title":"Create a Cluster","text":"<ul> <li>Run the following to create a cluster with Ingress ports bound:</li> </ul> <pre><code>cat &lt;&lt;EOF | kind create cluster --name local --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 7080\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 7433\n    protocol: TCP\nEOF\n</code></pre> <p>The cluster will be named 'kind-local'</p> Fish shell equivalent <pre><code>kind create cluster --name local --config (echo '\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 7080\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 7433\n    protocol: TCP\n' | psub)\n</code></pre> <p>Cluster services will be accessible under http://localhost:7080</p> <p>Change the hostPort variable if you wish to use a different port.</p>"},{"location":"dev-guide/containers/kubernetes-local-kind/#deploy-the-ingress-controller","title":"Deploy the Ingress Controller","text":"<p>Contour uses Envoy and may be a good choice in production.</p> <p>But the simplest and most battle tested in the Nginx ingress.</p> <p>Deploy with:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n</code></pre> <p>Check when it is ready:</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n</code></pre>"},{"location":"dev-guide/containers/kubernetes-local-kind/#test-the-ingress","title":"Test the Ingress","text":"<p>Run test service:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --filename=-\nkind: Pod\napiVersion: v1\nmetadata:\n  name: foo-app\n  labels:\n    app: foo\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: foo-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: foo-service\nspec:\n  selector:\n    app: foo\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: bar-app\n  labels:\n    app: bar\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: bar-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: bar-service\nspec:\n  selector:\n    app: bar\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /foo(/|$)(.*)\n        backend:\n          service:\n            name: foo-service\n            port:\n              number: 8080\n      - pathType: Prefix\n        path: /bar(/|$)(.*)\n        backend:\n          service:\n            name: bar-service\n            port:\n              number: 8080\n---\nEOF\n</code></pre> Fish shell equivalent <pre><code>kubectl apply --filename (echo '\nkind: Pod\napiVersion: v1\nmetadata:\n  name: foo-app\n  labels:\n    app: foo\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: foo-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: foo-service\nspec:\n  selector:\n    app: foo\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: bar-app\n  labels:\n    app: bar\nspec:\n  containers:\n  - command:\n    - /agnhost\n    - netexec\n    - --http-port\n    - \"8080\"\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\n    name: bar-app\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: bar-service\nspec:\n  selector:\n    app: bar\n  ports:\n  # Default port used by the image\n  - port: 8080\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: /foo(/|$)(.*)\n        backend:\n          service:\n            name: foo-service\n            port:\n              number: 8080\n      - pathType: Prefix\n        path: /bar(/|$)(.*)\n        backend:\n          service:\n            name: bar-service\n            port:\n              number: 8080\n---\n' | psub)\n</code></pre> <p>$ curl localhost:7080/foo/hostname</p> <p>should output \"foo-app\"</p> <p>$ curl localhost:7080/bar/hostname</p> <p>should output \"bar-app\"</p> <p>Cleanup test resources:</p> <pre><code>kubectl delete pod foo-app\nkubectl delete pod bar-app\nkubectl delete svc foo-service\nkubectl delete svc bar-service\nkubectl delete ingress example-ingress\n</code></pre>"},{"location":"dev-guide/containers/production-db/","title":"Production Databases","text":"<ul> <li>Production databases are best managed via Kubernetes.</li> <li>Generally the install and management of databases is maintained by a   Kubernetes 'Controller'.</li> <li>The controller simply manages the lifecycle of the database cluster,   allowing for replication, load balancing etc.</li> <li>There are a few options (in order of preference, 2024):<ul> <li>CloudNativePG</li> <li>CrunchyPG</li> <li>Zalando</li> <li>Percona, Stolon, KubeDB, etc</li> </ul> </li> </ul>"},{"location":"dev-guide/containers/production-db/#cloudnativepg","title":"CloudNativePG","text":"<p>This guide assumes you have a functioning Kubernetes cluster, plus available command line tools <code>kubectl</code> and <code>helm</code>.</p> <p>CloudNativePG makes four types of resources available:</p> <ul> <li>Cluster - a replicated database cluster.</li> <li>Pooler - load balancing on top of a cluster (pg_bouncer).</li> <li>Backup - on demand db backup.</li> <li>ScheduledBackup - regular scheduled db backup.</li> </ul> <p>These resources can be used in any namespace of the cluster, so a database could be deployed in a namespace alongside a tool.</p>"},{"location":"dev-guide/containers/production-db/#operator-install","title":"Operator Install","text":"<p>Via Helm:</p> <pre><code>helm upgrade --install cnpg \\\n    --namespace cnpg-system \\\n    --create-namespace \\\n    cnpg/cloudnative-pg\n</code></pre>"},{"location":"dev-guide/containers/production-db/#install-kubectl-plugin","title":"Install Kubectl Plugin","text":"<ul> <li>To simplify management of databases, it's best to install the cnpg plugin.</li> <li>Details on their   documentation site</li> <li>This allows us to easily do maintenance tasks such as backup, re-scale   replicas, delete specific replicas, upgrade the db image, etc.</li> </ul>"},{"location":"dev-guide/containers/production-db/#create-a-postgis-database","title":"Create a PostGIS Database","text":"<ul> <li>Now we have the operator installed, this allows us to create databases   in Kubernetes using <code>apiVersion: postgresql.cnpg.io/v1</code> and <code>kind: Cluster</code>.</li> <li>If we deploy a spec with this properties, a database will automatically   be deployed and managed by CloudNativePG.</li> <li>There are many configuration options available under the <code>spec</code> key.</li> </ul> <p>Example:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --filename=-\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: fmtm-db\nspec:\n  instances: 3\n  imageName: ghcr.io/cloudnative-pg/postgis:16\n  bootstrap:\n    initdb:\n      database: fmtm\n      postInitTemplateSQL:\n        - CREATE EXTENSION postgis;\n        - CREATE EXTENSION postgis_topology;\n        - CREATE EXTENSION fuzzystrmatch;\n        - CREATE EXTENSION postgis_tiger_geocoder;\n  storage:\n    size: 1Gi\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://fmtm-db-backups/\"\n      endpointURL: \"https://s3.fmtm.hotosm.org\"\n      s3Credentials:\n        accessKeyId:\n          name: fmtm-s3-creds\n          key: ACCESS_KEY_ID\n        secretAccessKey:\n          name: fmtm-s3-creds\n          key: ACCESS_SECRET_KEY\n    retentionPolicy: \"90d\"\nEOF\n</code></pre> Fish shell equivalent <pre><code>kubectl apply --filename (echo '\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: fmtm-db\nspec:\n  instances: 3\n  imageName: ghcr.io/cloudnative-pg/postgis:16\n  bootstrap:\n    initdb:\n      database: fmtm\n      postInitTemplateSQL:\n        - CREATE EXTENSION postgis;\n        - CREATE EXTENSION postgis_topology;\n        - CREATE EXTENSION fuzzystrmatch;\n        - CREATE EXTENSION postgis_tiger_geocoder;\n  storage:\n    size: 1Gi\n  backup:\n    barmanObjectStore:\n      destinationPath: \"s3://fmtm-db-backups/\"\n      endpointURL: \"https://s3.fmtm.hotosm.org\"\n      s3Credentials:\n        accessKeyId:\n          name: fmtm-s3-creds\n          key: ACCESS_KEY_ID\n        secretAccessKey:\n          name: fmtm-s3-creds\n          key: ACCESS_SECRET_KEY\n    retentionPolicy: \"90d\"\n' | psub)\n</code></pre> <p>Notes:</p> <ul> <li>This will use the latest PostGIS version.<ul> <li>For Postgres 16 this is PostGIS 3.4.</li> </ul> </li> <li>For testing, the <code>backup</code> section can be removed.</li> <li>Full API reference for options (v1.21) on the   docs site</li> </ul> <p>Alternatively, generate the spec using the <code>cnpg</code> plugin:</p> <pre><code>kubectl cnpg install generate \\\n  -n fmtm \\\n  --replicas 3\n</code></pre> <p>To delete the cluster, simply run the same command but use <code>kubectl delete</code> instead of <code>kubectl apply</code>.</p>"},{"location":"dev-guide/containers/production-db/#get-the-db-credentials","title":"Get the DB Credentials","text":"<ul> <li>By default the database is created in the controller namespace <code>cnpg-system</code>.</li> <li>A <code>postgres</code> password and user <code>password</code> are generated as secrets.</li> <li>To retrieve them:</li> </ul> <pre><code>kubectl get secret fmtm-db-app -o jsonpath='{.data.password}' | base64 -d\n\nkubectl get secret fmtm-db-app -o jsonpath='{.data.pgpass}' | base64 -d\n</code></pre> <p>In production the <code>superuserSecret</code> and <code>initdb.secret</code> spec values can be set to use a secret from a centralised secret manager.</p>"},{"location":"dev-guide/containers/production-db/#connection-pooling","title":"Connection Pooling","text":"<ul> <li><code>pg_bouncer</code> can be placed in front with a config such as:</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl apply --filename=-\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: fmtm-pooler-rw\nspec:\n  cluster:\n    name: fmtm-db\n  instances: 3\n  type: rw\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\nEOF\n</code></pre> Fish shell equivalent <pre><code>kubectl apply --filename (echo '\napiVersion: postgresql.cnpg.io/v1\nkind: Pooler\nmetadata:\n  name: fmtm-pooler-rw\nspec:\n  cluster:\n    name: fmtm-db\n  instances: 3\n  type: rw\n  pgbouncer:\n    poolMode: session\n    parameters:\n      max_client_conn: \"1000\"\n      default_pool_size: \"10\"\n' | psub)\n</code></pre>"},{"location":"dev-guide/containers/production-db/#scheduled-backups","title":"Scheduled Backups","text":"<p>To run a daily scheduled backup to S3:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --filename=-\napiVersion: postgresql.cnpg.io/v1\nkind: ScheduledBackup\nmetadata:\n  name: fmtm-backup\nspec:\n  schedule: \"0 0 0 * * *\"\n  backupOwnerReference: self\n  cluster:\n    name: fmtm-db\n  target: prefer-standby\n  method: barmanObjectStore\nEOF\n</code></pre> Fish shell equivalent <pre><code>kubectl apply --filename (echo '\napiVersion: postgresql.cnpg.io/v1\nkind: ScheduledBackup\nmetadata:\n  name: fmtm-backup\nspec:\n  schedule: \"0 0 0 * * *\"\n  backupOwnerReference: self\n  cluster:\n    name: fmtm-db\n  target: prefer-standby\n  method: barmanObjectStore\n' | psub)\n</code></pre>"},{"location":"dev-guide/containers/production-db/#restoring-the-backup","title":"Restoring The Backup","text":"<p>A new cluster can be deployed, using a backup source:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply --filename=-\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: fmtm-db\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n  bootstrap:\n    recovery:\n      source: clusterBackup\n  externalClusters:\n    - name: clusterBackup\n      barmanObjectStore:\n        destinationPath: \"s3://fmtm-db-backups/\"\n        endpointURL: \"https://s3.fmtm.hotosm.org\"\n        s3Credentials:\n          accessKeyId:\n            name: fmtm-s3-creds\n            key: ACCESS_KEY_ID\n          secretAccessKey:\n            name: fmtm-s3-creds\n            key: ACCESS_SECRET_KEY\nEOF\n</code></pre> Fish shell equivalent <pre><code>kubectl apply --filename (echo '\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: fmtm-db\nspec:\n  instances: 3\n  storage:\n    size: 1Gi\n  bootstrap:\n    recovery:\n      source: clusterBackup\n  externalClusters:\n    - name: clusterBackup\n      barmanObjectStore:\n        destinationPath: \"s3://fmtm-db-backups/\"\n        endpointURL: \"https://s3.fmtm.hotosm.org\"\n        s3Credentials:\n          accessKeyId:\n            name: fmtm-s3-creds\n            key: ACCESS_KEY_ID\n          secretAccessKey:\n            name: fmtm-s3-creds\n            key: ACCESS_SECRET_KEY\n' | psub)\n</code></pre>"},{"location":"dev-guide/containers/production-db/#accessing-exposing-the-db","title":"Accessing / Exposing The DB","text":"<ul> <li>Note that exposing the database via a URL is typically not recommended   and is generally not required.</li> <li>To connect to the database from an application deployed in the same cluster   we can use the service URL:   <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;service-port&gt;</code></li> <li>Instead, if external access is in fact required, we can use an Nginx Ingress.</li> <li>Documentation for this is available in   CloudNativePG docs</li> </ul>"},{"location":"dev-guide/repo-management/dependencies/","title":"Dependency Management","text":""},{"location":"dev-guide/repo-management/dependencies/#javascript","title":"Javascript","text":"<p>This section will be brief.</p> <p>Javascript already has a great dependency management ecosystem.</p> <p>Tools like NPM, Yarn, PNPM all work in a Node environment to solve dependency version compatibility and install them locally within your repo, in a <code>node_modules</code> directory.</p> <p>The only recommendation would be to use PNPM, as it has a much neater concept then the other two tools. Dependency locking is fast, and package installation is shared across repositories using a global package store on your system.</p> <p>This sets the standard for dependency management that some Python tools have attempted to replicate.</p>"},{"location":"dev-guide/repo-management/dependencies/#python","title":"Python","text":""},{"location":"dev-guide/repo-management/dependencies/#tldr","title":"tl;dr","text":"<ul> <li>Use <code>pyproject.toml</code> over <code>requirements.txt</code> and separate config files.</li> <li>Use uv to solve the depenedencies for your   packages.</li> <li>Lock with <code>&gt;=</code> for libraries, and <code>==</code> for applications.</li> <li>Upper bound caping should not be used, e.g. <code>&lt;=x.x.x</code>.</li> </ul>"},{"location":"dev-guide/repo-management/dependencies/#using-pyprojecttoml","title":"Using pyproject.toml","text":"<p>As per PEP 621, most information about a Python package should live in a file <code>pyproject.toml</code> in the root of your package (often the root of the repo, except in a monorepo setup).</p> <p>This file contains information on required dependencies (no more <code>requirement.txt</code> files), configuration for build tool, linters, test suites (pytest), etc.</p>"},{"location":"dev-guide/repo-management/dependencies/#dependency-solvers","title":"Dependency Solvers","text":"<p>uv is recommended.</p> <p>Their docs are quite comprensive, so we won't duplicate here.</p> <p>Main reasoning:</p> <ul> <li>Full support for current PEP standards.</li> <li>Replaces multiple tools in one (pip, pyenv, twine, venv).</li> <li>Ridiculously fast dependency solving (seconds).</li> <li>Very comprehensive feature set, active community, and full   time paid developers working on it (funded by astraL.sh).</li> </ul>"},{"location":"dev-guide/repo-management/dependencies/#locking-dependency-versions","title":"Locking Dependency Versions","text":"<p>Generally it is good practice to pin the version of an underlying dependency you use in your code. This helps to prevent future breakage.</p> <p>However, there is an important distinction between two types of package:</p> <ul> <li>Libraries: An underlying Python module that is used within another tool.</li> <li>Applications: A software tool. Typically not installable.   Such as HOT's web APIs that underpin it's tools   (raw-data-api, TM, FMTM, etc).</li> </ul> <p>Capping upper limits for library dependencies has long term negative effects, and should never be taken lightly.</p>"},{"location":"dev-guide/repo-management/dependencies/#locking-for-applications","title":"Locking for Applications","text":"<p>An application would sit at the highest level in the chain of installed dependencies: it uses underlying libraries/packages, but is not installed itself.</p> <p>There are three main options for pinning.</p>"},{"location":"dev-guide/repo-management/dependencies/#specific-versions","title":"Specific Versions \ud83d\udc4d","text":"<pre><code>pip install mydep==1.0.4\n</code></pre> <p>This is generally the recommended approach for reproducable environments.</p>"},{"location":"dev-guide/repo-management/dependencies/#approximate-versions","title":"Approximate Versions","text":"<pre><code>pip install mydep~=1.0.4\n</code></pre> <p>This will install &gt;1.0.4, but not increment the minor version.</p> <p>So the maximum installable version here would be 1.0.11, if this is the last version before the 1.2.x minor increment.</p> <p>This is an acceptable approach for some dependencies, but generally not recommended, as SEMVER is no guarantee of avoiding breakage (definitions are quite subjective, so patch versions can sometimes also introduce breakage).</p>"},{"location":"dev-guide/repo-management/dependencies/#minimum-versions","title":"Minimum Versions","text":"<pre><code>pip install mydep&gt;=1.0.4\n</code></pre> <p>This will install any version greater than that specified.</p> <p>This means breaking changes may be introduced if v2.0.0 is released.</p> <p>This is not recommended for applications, as installing one day may work, then break the next day due a dependency update.</p>"},{"location":"dev-guide/repo-management/dependencies/#locking-for-libraries-packages","title":"Locking for Libraries / Packages","text":"<p>Sometimes we develop a package that is used as a dependency in other tools.</p> <p>Examples would be:</p> <ul> <li>osm-login-python</li> <li>osm-fieldwork</li> <li>osm-rawdata</li> <li>fmtm-splitter</li> </ul> <p>In these cases, the packages requires underlying dependencies to function.</p> <p>The packages should never have pinned dependencies to a specific version.</p> <p>It is recommended that versions should be pinned in an open ended way, using greater than or equal too (&gt;=).</p> <p>For this to work, a minimum required version of a dependency should be established. There is little point pinning &gt;= if a very up to date version of a dependency is used (forcing the installer to update to a very recent version).</p> <p>This ensures that a minimum version of the dependency is used, but does not prevent dependency upgrades for those using the package.</p> <p>Using approximate pinning (~=) may also be possible, however, this assumes too much about future compatibility, which is something that is difficult to predict.</p> <p>For longevity, it is best to provide the most flexible option for dependency solvers: &gt;=.</p> <p>Only add a cap if a dependency is known to be incompatible or there is a high (&gt;75%) chance of it being incompatible in its next release. An example of a library that should probably be capped is GDAL.</p> <p>Some more in depth technical reading can be found in this useful breakdown.</p>"},{"location":"dev-guide/repo-management/git/","title":"Git and Pre-Commit Hooks","text":""},{"location":"dev-guide/repo-management/git/#git-the-basics","title":"Git - The Basics","text":"<p>Is is a tool we all know and love.</p> <p>There are a few basic concepts that would be good to master.</p>"},{"location":"dev-guide/repo-management/git/#development-lifecycle","title":"Development Lifecycle","text":"<p>The concept of Continuous Integration means that code is continually developed and merged into a branch, for testing or deployment.</p> <p>CI/CD:</p> <ul> <li>CI: merge frequently into master.<ul> <li>Minimise merge issues from multiple collaborators.</li> </ul> </li> <li>CD: deploy master frequently to production.<ul> <li>Regular updates for new features and bug fixes.</li> </ul> </li> </ul> <p>The primary branches, in order:</p> <ul> <li>Development: the branch that fixes or features are continually merged.</li> <li>Staging: where development branch fixes/features are grouped and tested   together, with the intention to push to production once QA/QC passes.</li> <li>Production: where staging is stabilised and released at intervals as   actual versions of your tool/software.</li> </ul> <p>Additional supporting branches:</p> <ul> <li>Fix: fixes a bug or issue.</li> <li>Feature: add a new feature that didn't exist before.</li> <li>Hotfix: if an issue is found after a production release is made, a hotfix   can be used to patch the production code.</li> </ul>"},{"location":"dev-guide/repo-management/git/#git-flow","title":"Git Flow","text":"<p>Git flow is a branching model.</p> <p>The most basic version of this would be:</p> <p><code>feature</code> or <code>fix</code> --&gt; <code>main</code> (production)</p> <p></p> <p>Adding in the extra steps of the described development lifecycle, we get:</p> <p><code>feature</code> or <code>fix</code> --&gt; <code>development</code> --&gt; <code>staging</code> --&gt; <code>main</code> (production)</p> <p></p> <p>The number of additional stages can be flexible depending on requirements.</p>"},{"location":"dev-guide/repo-management/git/#forking","title":"Forking","text":"<p>To work on an open-source repository, generally you may not have direct access to the repo from the start.</p> <p>A common pattern is to make your own copy of the repo, a 'fork' of it to work on.</p> <p>Within this repo you create a new branch:</p> <pre><code>git checkout -b feat/some-new-feature\n</code></pre> <p>Then when you push the branch to your fork, generally a code hosting platform like Github/Gitlab will prompt you to create a Pull Request or Merge Request (the same thing).</p>"},{"location":"dev-guide/repo-management/git/#pull-requests","title":"Pull Requests","text":"<p>A pull requests (PR) is used to merge the code from your forked repository into the original code repository.</p> <p>You should describe as accurately as possible what solution your code provides, or feature it adds, and why it is necessary.</p> <p>Ideally try to link it to an existing Issue in the repository issue board.</p> <p>The maintainer of the repo will review your code, comment, and merge it in.</p>"},{"location":"dev-guide/repo-management/git/#rebasing","title":"Rebasing","text":"<p>This is often a scary concept to many.</p> <p>It essentially re-writes the Git history on a branch, so use with care.</p> <p>Use case: sometimes your code gets out of sync with the target branch you originally branched from.</p> <p>For example you branched from <code>develop</code> to a branch <code>feat/some-new-thing</code>.</p> <p>If you wish to pull in the latest updates from the <code>develop</code> branch into your feature branch, you can do a rebase:</p> <pre><code>git checkout develop\ngit pull\ngit checkout feat/some-new-thing\ngit rebase develop\n</code></pre> <p>This will insert the updates below your code edits. I.e. the history will show your commits on top of the most recent <code>develop</code> commits.</p> <p>Visually this will be:</p> <p></p>"},{"location":"dev-guide/repo-management/git/#merge-vs-rebase","title":"Merge vs Rebase","text":"<ul> <li>Use merge for feature \u2192 main (work finished).<ul> <li>When on develop use merge to include a (idealy finished) feature</li> </ul> </li> <li>Use rebase for develop \u2192 feature (work in progress).<ul> <li>When on the feature branch use rebase from develop to include   the latest changes</li> <li>Regularly rebasing feature branches will keep them up to   date with current features (either from main or develop).</li> <li>This means that conflicts can be resolved gradually,   instead of in one go during a merge.</li> </ul> </li> </ul> <p>The Perils of Rebase</p> <p>The most important take away: rebase is a powerful tool, but be wary using it if you are collaborating with someone on the same feature branch.</p> <p>If a teammate happens to rebase a branch that you are working on, the easiest solution is to stash and reset to get the rebased edits:</p> <pre><code>git stash -u\ngit fetch origin feat/new-thing\ngit reset --hard feat/new-thing\ngit stash apply\ngit stash drop\n</code></pre>"},{"location":"dev-guide/repo-management/git/#anticipating-a-merge","title":"Anticipating a Merge","text":"<ul> <li>Often in a developers workflow, they create on PR, then while waiting for review   work on another PR.</li> <li>If <code>PR-2</code> relies on work from <code>PR-1</code> to be merged, this can be an issue.</li> <li>One approach to solving this dilemma is informally called 'anticipating a merge'.</li> </ul>"},{"location":"dev-guide/repo-management/git/#anticipating-a-merge-workflow","title":"Anticipating a Merge Workflow","text":"<ol> <li> <p>Complete work on branch <code>PR-1</code> and push to create a PR.</p> </li> <li> <p>Ensure you are on branch <code>PR-1</code>:</p> <pre><code>git checkout pr-1\n</code></pre> </li> <li> <p>Create branch <code>PR-2</code> based off <code>PR-1</code>:</p> <pre><code>git checkout -b pr-2\n</code></pre> </li> <li> <p>Build your feature on top of the code in <code>PR-1</code>.</p> </li> <li>Complete work on branch <code>PR-2</code> and push to create a PR.</li> </ol> <p>Important Notes:</p> <ul> <li>The new <code>PR-2</code> will initially include the commits from <code>PR-1</code>.</li> <li>However, the commits shown in the PR changelog will disappear once <code>PR-1</code> is merged   into the target branch (e.g. <code>development</code>).</li> <li>You should mention in the <code>PR-2</code> description that this PR 'relies on PR-1 being   accepted and merged'.</li> </ul>"},{"location":"dev-guide/repo-management/git/#updating-pr-1-while-pr-2-is-in-progress","title":"Updating PR-1 While PR-2 Is In Progress","text":"<ul> <li>You may encounter a situation where the review of <code>PR-1</code> takes some time.</li> <li>If <code>PR-2</code> makes progress, but the reviewer says that <code>PR-1</code> requires updates.</li> <li>In this case you have two options:<ul> <li>Merge <code>PR-1</code> and add the required updates to <code>PR-2</code>.</li> <li>Update <code>PR-1</code> and merge, then rebase <code>PR-2</code> against the target branch.</li> </ul> </li> </ul>"},{"location":"dev-guide/repo-management/licensing/","title":"Licensing","text":"<p>With so many options for licensing code, the topic can be quite confusing. Many projects like Boost, Eclipse, Apache, etc.. have their own license, often based on the GPL, which only apply to that project, so aren't appropriate for use at HOT. There are two primary types of licenses, permissive and non-permissive. Non permissive licenses may appear to be open source, but often it's a few clauses in the licenses that make them unable to be used in other open source projects.</p> <p>HOT believes strongly in the freedoms embodied on the free software culture, while following the collaborative development processes common for open source software projects.</p> <p>All of the software at HOT uses permissive licenses, mostly the GPLv3 or AGPLv3, with some older code using the BSD license.</p>"},{"location":"dev-guide/repo-management/licensing/#the-gplv3-and-agplv3","title":"The GPLv3 and AGPLv3","text":"<ul> <li>These are strong 'copyleft' licences, meaning that anyone   modifiying the software is required to make those changes openly   available. This is commonly called the derivative work clause. Any   derivative software must be contributes it back to the original   software project with no restrictions.</li> </ul> <ul> <li>The main purpose of this license is to prevent commercial   exploitation of open code, making any modifications open so the   entire community can benefit.</li> </ul> <ul> <li>The distinction between the two is that AGPL is used for code   distributed via a network (i.e. a website), whereas, GPL can be   used for all other code (e.g. PyPi packages, compiled code).</li> </ul> <p>Originally HOT was using BSD-2-Clause, which is more permissive and not copyleft, meaning it is fully open but there is no protection from commercial exploitation.</p> <p>Tools using BSD-2-Clause:</p> <ul> <li>Tasking Manager</li> <li>Export Tool</li> <li>Open Aerial Map</li> </ul>"},{"location":"dev-guide/repo-management/licensing/#other-content-is-cc-by","title":"Other Content is CC-BY","text":"<ul> <li>Creative content, such as translations and designs are copyrighted   different.</li> <li>As with AGPL for code, this license is 'copyleft'.</li> <li>It allows others modify and built upon our own, even for commercial   purposes, as long as they credit us and   license their work under the same terms as CC-BY.</li> <li>The current CC-BY license available would be   Creative Commons Attribution 4.0 International.</li> </ul>"},{"location":"dev-guide/repo-management/licensing/#what-licenses-can-we-use","title":"What Licenses Can We Use","text":"<ul> <li>We use many underlying packages and tools within our software.</li> <li>Any contributor must be aware of licensing before adding any   additional external code to our software.</li> </ul>"},{"location":"dev-guide/repo-management/licensing/#compatible-licenses","title":"Compatible Licenses","text":"<p>This is a short list of copyleft compatible licenses. For more detail, refer to the Free Software Foundation (FSF) page on compatible and incompatible licenses. This in far from an exhaustive list, only the most common ones are listed here.</p> <ul> <li>Public Domain</li> <li>Mozilla Public License (MPL) version 2.0</li> <li>Apache License 2.0</li> <li>Modified BSD license</li> <li>Intel Open Source License</li> <li>FreeBSD license</li> </ul>"},{"location":"dev-guide/repo-management/licensing/#uncertain-licenses","title":"Uncertain Licenses","text":"<p>Being technologists, and not lawyers, we do not have the time or expertise to identify and assess every license type available.</p> <p>There are some licenses that could potentially be compatible, but we do not have a well formed opinion about their usage. Typically we would defer to an authority such as the FSF.</p> <p>These licenses may include, but are not limited to:</p> <ul> <li>EUPL-1.2</li> </ul> <p>For the moment we do not expressly forbid their usage, but the rationale for doing so should be explained on a case-by-case basis.</p>"},{"location":"dev-guide/repo-management/licensing/#incompatible-licenses","title":"Incompatible Licenses","text":"<p>The Open Source Initiative also has a list of approved open source licenses. Not all of these are compatible with copylefted software, so may not be appropriate for code at HOT even though they are considered permissive licenses.</p> <p>Licenses that can't be used for any code in HOT software projects:</p> <ul> <li>Mozilla Public License (MPL) version 1.1</li> <li>Apache License, Version 1.0 and 1.1</li> <li>Apple Public Source License (APSL), version 2</li> <li>Microsoft Public License (Ms-PL)</li> <li>NASA Open Source Agreement</li> <li>Original BSD license</li> <li>Common Development and Distribution License (CDDL), version 1.0</li> <li>Common Public Attribution License 1.0 (CPAL)</li> <li>Common Public License Version 1.0</li> <li>European Union Public License (EUPL) version 1.1 and 1.2</li> <li>IBM Public License, Version 1.0</li> <li>Open Software License, all versions through 3.0</li> </ul>"},{"location":"dev-guide/repo-management/monorepos/","title":"Using Monorepos","text":"<ul> <li>It may seem intuitive to divide up each part of a project into logical   components / separate Git repositories (backend / frontend as an example).</li> <li> <p>However, there are many benefits to monorepos (all in a single repository),   and they often allow for faster development cycles.</p> <p>At HOT, we mostly prefer a monorepo-based setup</p> </li> </ul>"},{"location":"dev-guide/repo-management/monorepos/#advantages-of-monorepos","title":"Advantages Of Monorepos","text":"<ul> <li>Discoverability: particularly when working with an open-source community   around a tool. Having all code in one place makes for an easier entrypoint   for potential contributors.</li> <li>Easier management: manage issues and discussions about a tool in one place,   including documentation.</li> <li>Component compatibility: having frontend and backend components in one   place ensures that versions of both should always be compatible with   one another (especially when released together).</li> <li>Better collaboration: avoids silos for 'backend' and 'frontend' teams.</li> <li>Easier testing: for full E2E tests involving both the frontend and backend,   it is often easier running the code from a single repo (e.g. start the   backend first, then run API calls from the frontend).</li> <li>Easier deployment: tightly coupling releases for components and sharing   CI/CD workflows for the testing and deployment.</li> <li>Standardized tooling: every developer uses the same tools for a project.   Hopefully, this avoids 'it works on my machine' issues.</li> </ul>"},{"location":"dev-guide/repo-management/monorepos/#uv-workspaces-python","title":"UV Workspaces (Python)","text":"<ul> <li>If working on a Python project with many self-contained, packageable   modules, <code>uv</code> workspaces are an excellent tool for managing and   publishing multiple modules from the same repository. This is   similar to Cargo in Rust.</li> </ul>"},{"location":"dev-guide/repo-management/monorepos/#when-to-use-separate-repos","title":"When To Use Separate Repos","text":"<ul> <li>Very large teams: in organizations with 100+ developers, monorepos can   become difficult to manage, leading to bottlenecks in workflows such as   CI/CD and version control conflicts.</li> <li>Unrelated projects: if different parts of a system have little or no   dependency on each other (e.g. a backend API unrelated to a frontend app),   splitting them into separate repositories might be more logical.</li> <li>Scalability concerns: as a monorepo grows, issues can arise with   repository size, build times, and dependency management. Modular   repositories can scale better over time.</li> <li>Using entirely different technology stacks between projects.</li> </ul>"},{"location":"dev-guide/repo-management/pre-commit/","title":"Pre-Commit Hooks","text":""},{"location":"dev-guide/repo-management/pre-commit/#git-hooks","title":"Git Hooks","text":"<p>Git has hooks, which can run code on various events:</p> <ul> <li>Prior to commiting code (pre-commit).</li> <li>After committing code (post-commit).</li> <li>After checking out a branch (post-checkout).</li> <li>Before rebasing (pre-rebase).</li> <li>Etc.</li> </ul>"},{"location":"dev-guide/repo-management/pre-commit/#pre-commit-hook","title":"Pre-Commit Hook","text":"<ul> <li>Pre-commit Git hooks run checks before a commit is accepted.</li> <li>Pre-commit is a package to automate the setup.</li> <li>After config, issues such as syntax errors and security flaws are picked up.</li> <li>Used mostly for formatting and linting.</li> <li>Linting is checking code against a set of formatting rules   and for syntax errors.</li> </ul>"},{"location":"dev-guide/repo-management/pre-commit/#pre-commit-python-tool","title":"Pre-Commit (Python Tool)","text":"<ul> <li>Pre-commit is a Python tool for simplifying   and applying Git pre-commit hooks.</li> <li>Hooks can be configured via a YAML file, then applied on each attempted commit.</li> <li>There are many hooks available from different sources.</li> </ul> <p>Install it with:</p> <pre><code>pip install pre-commit\n</code></pre>"},{"location":"dev-guide/repo-management/pre-commit/#add-pre-commit-configyaml","title":"Add pre-commit-config.yaml","text":"<ul> <li>Add a <code>pre-commit-config.yaml</code> to your repo root.</li> <li>A best practice config file, taken from FMTM:</li> </ul> <pre><code>repos:\n  # Versioning: Commit messages &amp; changelog\n  - repo: https://github.com/commitizen-tools/commitizen\n    rev: v3.28.0\n    hooks:\n      - id: commitizen\n        stages: [commit-msg]\n\n  # Lint / autoformat: Python code\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: \"v0.5.4\"\n    hooks:\n      # Run the linter\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n      # Run the formatter\n      - id: ruff-format\n\n  # Autoformat: YAML, JSON, Markdown, etc.\n  - repo: https://github.com/pycontribs/mirrors-prettier\n    rev: v3.3.3\n    hooks:\n      - id: prettier\n        args:\n          [\n            --ignore-unknown,\n            --no-error-on-unmatched-pattern,\n            \"!CHANGELOG.md\",\n            \"!CONTRIBUTING.md\",\n            \"!src/frontend/pnpm-lock.yaml\",\n          ]\n\n  # Lint: Bash scripts\n  - repo: https://github.com/openstack-dev/bashate.git\n    rev: 2.1.1\n    hooks:\n      - id: bashate\n\n  # Lint: Shell scripts\n  - repo: https://github.com/shellcheck-py/shellcheck-py\n    rev: v0.9.0.6\n    hooks:\n      - id: shellcheck\n        args: [\"-x\"]\n\n  # Lint: Markdown\n  - repo: https://github.com/igorshubovych/markdownlint-cli\n    rev: v0.41.0\n    hooks:\n      - id: markdownlint\n        args: [--fix, --ignore, CHANGELOG.md, --ignore, .github]\n</code></pre> <p>Note: the config above is for a monorepo configuration.</p> <p>Your repo may not require both Python and JS code formatting.</p>"},{"location":"dev-guide/repo-management/pre-commit/#add-hooks","title":"Add Hooks","text":"<p>Run</p> <pre><code># Standard install for most hooks\npre-commit install\n\n# Additional commit-msg hook (for the commitizen hook above)\npre-commit install --hook-type commit-msg\n</code></pre> <p>Now when you attempt to commit to the repo:</p> <ul> <li>Code will be auto-formatted.</li> <li>An error will show if linting fails.</li> <li>An error will show if commit messages are in the wrong format.</li> </ul>"},{"location":"dev-guide/repo-management/version-control/","title":"Version Control","text":""},{"location":"dev-guide/repo-management/version-control/#semantic-versioning-semver","title":"Semantic Versioning (SemVer)","text":"<ul> <li>A method of versioning your software packages.</li> <li>Makes most sense for versioning software that is used by another piece   of software, e.g. a Python package that is installed.</li> <li>The version number is composed of three segments: <code>MAJOR.MINOR.PATCH</code>,   for example <code>1.4.2</code>.<ul> <li>MAJOR: Increments when incompatible API changes are introduced.   It indicates that there are significant and potentially breaking   changes that might require modifications in existing code.</li> <li>MINOR: Increments when new features are added in a backward-compatible manner.   It signals the addition of functionality without breaking existing code.   Developers can expect that their code will still work as intended.</li> <li>PATCH: Increments for backward-compatible bug fixes.   It denotes minor improvements or bug fixes that do not introduce new   features and do not break existing functionality.</li> </ul> </li> </ul>"},{"location":"dev-guide/repo-management/version-control/#calver-and-others","title":"CalVer and others","text":"<ul> <li>Sometimes SemVer doesn't make sense. For example, in a user-facing   software service/tool, what does a 'breaking' change denote?</li> <li>In these cases, other conventions may be better suited.</li> <li>At HOT, we use <code>YYYY.VERSION.PATCH</code>, in line with the developers of the ODK ecosystem.<ul> <li>YYYY: the current year.</li> <li>VERSION: the release version for the current year. Resets in the next year.</li> <li>PATCH: a patch to the released version, typically bugfixes.</li> </ul> </li> </ul>"},{"location":"dev-guide/repo-management/version-control/#conventional-commits","title":"Conventional Commits","text":"<p>A specification for adding human and machine readable meaning to commit messages.</p> <p>A 'new' standard for commit messages.</p> <p>Format: <code>&lt;type&gt;[optional scope]: &lt;description&gt;</code></p> <p>Example <code>feat: allow provided config object to extend other configs</code> Example <code>fix: fixed the bug in issue #123</code></p> <p>Advantage: Automated SemVer version management (major.minor.patch), and automated changelogs.</p>"},{"location":"dev-guide/repo-management/version-control/#tool-commitizen","title":"Tool: Commitizen","text":"<ul> <li>Commitizen is a   Python tool to help with creating conventional commits and   automating version control.</li> </ul> <ul> <li>Versions are managed by Commitizen from the <code>pyproject.toml</code> file in a   repo.</li> </ul> <ul> <li>Versions are determined by conventional commit messages:<ul> <li><code>fix: xxx</code> denotes a patch, <code>feat: xxx</code> denotes a minor increment.</li> <li>Breaking changes are applied every year to increment the <code>YYYY</code> in   place of <code>MAJOR</code>.</li> </ul> </li> </ul>"},{"location":"dev-guide/repo-management/version-control/#install","title":"Install","text":"<p><code>pip install commitizen</code></p>"},{"location":"dev-guide/repo-management/version-control/#commiting-code","title":"Commiting Code","text":"<ul> <li>Instead of <code>git commit</code> use <code>cz commit</code> and follow the prompts.</li> <li>You can select the type of commit, plus additional metadata.</li> </ul>"},{"location":"dev-guide/repo-management/version-control/#bumping-a-version","title":"Bumping a Version","text":"<ul> <li> <p>When you decide it is time to create a new version:</p> <pre><code>pip install commitizen # (if not installed)\n\ncz bump --check-consistency --changelog\n\ngit push\ngit push --tag\n</code></pre> </li> </ul> <p>Warning</p> <p>This assumes you have repo write access and are working on the main branch.</p> <p>This will:</p> <ul> <li>Update the SemVer version number in locations specific in <code>pyproject.toml</code>,   throughout the codebase.<ul> <li>If a <code>feat</code> commit is included, the version is bumped by a minor   increment (0.x.0), if only <code>fix</code> is included a patch will be used   (0.0.x).</li> </ul> </li> <li>Automatically update CHANGELOG.md with all changes since the last version.</li> <li>Create a tag matching the version number.</li> </ul> <p>Tip</p> <p>Oh no I made a mistake!</p> <p>Worry not, the version increment can be reverted:</p> <pre><code># Revert the commit\ngit reset --soft HEAD~1\n\n# Delete the branch\ngit branch -d v0.x.x\ngit push --delete origin v0.x.x\n</code></pre>"},{"location":"dev-guide/repo-management/version-control/#tool-uv","title":"Tool: uv","text":"<ul> <li>Using workspaces it's possible to setup your Python packages in a   monorepo.</li> <li><code>uv</code> can be used to manage, and publish, each package independently.</li> <li> <p>To publish a specific package from a monorepo, go to the directory   where the <code>pyproject.toml</code> is located:</p> <pre><code>uv --project=./packages/project1 build\nuv --project=./packages/project1 publish\n</code></pre> </li> </ul>"},{"location":"dev-guide/repo-management/version-control/#creating-releases","title":"Creating Releases","text":"<ol> <li>Update the version throughout the code (Bumping a Version).</li> <li>Go to the Releases page of your repo    (https://github.com/ORG/REPO/releases).</li> <li>Click <code>Draft a new release</code>.</li> <li>Click <code>Choose a tag</code>, then input the current version number and press    enter (this will automatically create a matching tag for your release).</li> <li>Set the <code>Release title</code> to v<code>x.x.x</code>, replacing with your version number.</li> <li>Add a description if possible, then release.</li> </ol> <p>This should trigger the PyPi publishing workflow (for HOT repos), and your version will be available on PyPi.org.</p>"},{"location":"devops/aws-iam/","title":"AWS Roles And Permissions (IAM)","text":"<p>This can be a deep rabbit hole to go down - we will try and keep it simple!</p> <p>In general, we can consider these access options:</p> <ul> <li>IAM User: for people. Long-term access keys. Generally for manual   user usage.</li> <li>IAM Role: for code. Temporary credentials via OIDC, STS, etc,   for usage in things like Github workflows.</li> <li>Resource Policy: permissions directly on a service (e.g. S3 bucket).</li> <li>Other AWS-specific ways to login such as EC2 instance profiles   (granting an EC2 machine access to certain things by default),   or IAM roles for service accounts used with Kubernetes.</li> </ul> <p>It may be possible to use a bit of the above, and conflict can occur, so it's key to be consistent for what type of configuration you set.</p>"},{"location":"devops/aws-iam/#s3-bucket-permissions","title":"S3 Bucket Permissions","text":"<ul> <li>There are two key things to consider:<ol> <li>Bucket permissions: which bucket, and what permissions.</li> <li>CORS policy: which websites can access the bucket.</li> </ol> </li> </ul>"},{"location":"devops/aws-iam/#bucket-permissions","title":"Bucket Permissions","text":"<ul> <li> <p>The simplest way to access a bucket (e.g. from AWS CLI):</p> <ul> <li>Create an IAM user.</li> <li> <p>Grant the user permission to read/write the specific bucket.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n      \"Resource\": \"arn:aws:s3:::my-example-bucket/*\"\n    }\n  ]\n}\n</code></pre> </li> </ul> <ul> <li>Generate access credentials for the IAM user.</li> <li>Login to your terminal or app using the provided credentials.</li> </ul> </li> </ul> <ul> <li>For pushing to buckets from CI/CD workflows, it might be best   to use temporary credentials granted via   OIDC &amp; roles.</li> <li>For accessing a bucket from an EC2 instance,   EC2 instance profiles   may be best.</li> <li> <p>Example policy written directly on the bucket instead:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"NameThePolicyWhateverYouWant\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:role/my-ci-role\"\n      },\n      \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n      \"Resource\": \"arn:aws:s3:::my-example-bucket/*\"\n    }\n  ]\n}\n</code></pre> </li> </ul>"},{"location":"devops/aws-iam/#bucket-cors-policy","title":"Bucket CORS Policy","text":"<p>To simply allow access from all sites, use:</p> <pre><code>{\n  \"CORSRules\": [\n    {\n      \"AllowedOrigins\": [\"*\"],\n      \"AllowedMethods\": [\"GET\", \"HEAD\"],\n      \"AllowedHeaders\": [\"*\"]\n    }\n  ]\n}\n</code></pre> <p>To grant specific access to different sites, use:</p> <pre><code>{\n  \"CORSRules\": [\n    {\n      \"AllowedOrigins\": [\"https://example.com\", \"https://app.example.org\"],\n      \"AllowedMethods\": [\"GET\", \"HEAD\", \"PUT\"],\n      \"AllowedHeaders\": [\"Authorization\", \"Content-Type\"],\n      \"ExposeHeaders\": [\"ETag\"],\n      \"MaxAgeSeconds\": 3000\n    }\n  ]\n}\n</code></pre>"},{"location":"modules/backend/","title":"Backend Python Modules","text":""},{"location":"modules/backend/#osm-login-python","title":"OSM Login Python","text":"<p>A way to consistently implement OSM login (via underlying OAuth2) in our applications.</p>"},{"location":"modules/backend/#drone-flightplan","title":"Drone Flightplan","text":"<p>A module within DroneTM that can generate flightplans for various drone models standalone. Useful to integrating into workflows outside of DroneTM, such as within QGIS plugins.</p>"},{"location":"modules/backend/#pg-nearest-city","title":"PG Nearest City","text":"<p>A very simple tool for reverse-geocoding points to the nearest city over 1000 population. Requires no network requests and runs entirely in an attached PostGIS instance.</p>"},{"location":"modules/backend/#geojson-aoi-parser","title":"GeoJSON AOI Parser","text":"<p>Consistent parsing of user GeoJSON upload, for processing by tools such as raw-data-api, FieldTM, Drone-TM, etc, using an underlying PostGIS database.</p>"},{"location":"modules/backend/#raw-data-api-py","title":"Raw Data API Py","text":"<p>A Python wrapper for HOT's raw-data-api. This allows for downloading of frequently updated OSM data very conveniently.</p>"},{"location":"modules/backend/#osm-fieldwork","title":"OSM Fieldwork","text":"<p>Various utility programs useful for field data collection using ODK Central. These modules are used extensively in Field for all the backend data processing.</p>"},{"location":"modules/backend/#fieldtm-splitter","title":"FieldTM Splitter","text":"<p>A splitting algorithm using PostGIS to divide an idea into task areas, factoring in prominent map features (roads, rivers, etc).</p> <p>The division is configurable via various parameters.</p>"},{"location":"modules/backend/#fair-utilities","title":"fAIr Utilities","text":"<p>Various machine learning utils used within the fAIr backend.</p>"},{"location":"modules/frontend/","title":"Frontend Modularization","text":""},{"location":"modules/frontend/#shared-ui-components","title":"Shared UI Components","text":"<p>Consistent styling and component functionality across all of our projects.</p> <p>Currently in design and prototype stage, but will be rolled out for all projects eventually.</p> <p>link to the related docs</p>"},{"location":"modules/frontend/#shared-map-components","title":"Shared Map Components","text":"<p>We are contributing to and building out the svelte-maplibre package.</p> <p>This is built around MapLibre, with a goal to export the components to standard web components (custom elements) that could be embedded into any frontend framework.</p> <p>This would allow for a consistent declarative API for a web map library to be integrated into our tools.</p> <p>The plan would be to support various cloud optimised geo data formats:</p> <ul> <li>COG</li> <li>flatgeobuf</li> <li>GeoParquet</li> <li>PMTiles</li> </ul>"},{"location":"modules/frontend/#gcp-editor","title":"GCP Editor","text":"<p>HOT's GCP Editor is a web component that can be embedded into any web application.</p> <p>It's purpose is to mark Ground Control Points on collected aerial imagery, assisting the processing workflow for accurately matching collected imagery to known coordinates in the field.</p> <p>The goal is to work alongside the OpenDroneMap team and possibly have this as a shared component amongst tools.</p>"},{"location":"modules/frontend/#qr-codes","title":"QR Codes","text":"<p>Reads and decodes base64 and zlib encoded JSON data within a QRCode in Javascript</p>"},{"location":"projects/future-plans/","title":"Future Plans / Ideas","text":"<p>This page describes various things we want at HOT, and are either in the process of achieving, or have marked as potential future projects.</p> <p>If you are software developer and wish to contribute to something new, fun, and exciting, feel free to reach out about one of these ideas.</p>"},{"location":"projects/future-plans/#end-to-end-concept","title":"End-To-End Concept","text":"<ul> <li>We are hard at work building a full end-to-end mapping workflow, leveraging   our suite of available tools.</li> <li>A conceptual diagram of this can be seen on the   home page   of this site.</li> <li>The goal is to allow the mapping commmunity to go trhough the entire   mapping proces through community ownership and tools.   Tools in the end-to-end mapping worklfow include a spectrum of solutions one   can use depending on the need, not necessearily full set of tools.   This can include:<ul> <li>Creating a map, starting from absolutely nothing using the entire   suite of tools. Imagery --&gt; digitize (extract data) --&gt; field map --&gt; export.</li> <li>Choose a subset of our tools to fill the data gaps missing for   their map. For example, maybe they have great digitized features   already, but need to field-verify them and add extra tag info.</li> </ul> </li> </ul>"},{"location":"projects/future-plans/#a-centralized-e2e-site","title":"A centralized E2E Site","text":"<ul> <li>We need a centralized entrypoint for E2E mapping to make the workflow more accessilbe.</li> <li>Work a website to achieve this was started in   this project, however,   capacity has been limited to continue it so far.</li> <li>We envision to have a central place that users   can access, to:<ul> <li>Understand what E2E is, and what it enables.</li> <li>Showcase the tools used in E2E and their capabilities.</li> <li> <p>Some hand-holding tutorials to easily link together all the tools   required to successfully carry out an E2E mapping project.   For the latter, we target an interactive tutorial, where the user   can start by drawing an AOI for where they need to map,</p> <p>then each step will be explained, with helper buttons and visualisations to assist in the E2E mapping journey.</p> </li> </ul> </li> </ul>"},{"location":"projects/future-plans/#shared-login-accross-tools","title":"Shared Login accross tools","text":"<ul> <li>All of our tools need a shared login mechanism.</li> <li>Currently every tool has separate login, so a user visiting one tool   needs to log in again when accessing another in the E2E suite.</li> <li>A very nice to have feature would be seamless login between our   entire suite of tools:<ul> <li>Simple: the same auth cookie work across all apps, but user   information is managed per-app still. Users are seamlessly logged   in when moving between apps. This would be possible as every app   shares the domain hotosm.org.</li> <li>Hard: the authentication is managed somewhere centrally. The   user logs in and user details are stored in a centralised 'HOT' database.   The auth details are then passed to each application, but the app   doesn't store user data. This one is technically difficult and I   wouldn't advocate for it (we are essentially creating our own   OAuth provider). In this case I think it's better to centralise   around OSM auth.</li> </ul> </li> <li>This links in to shared ui components below,   as the login will likely be faciliated within a 'header' component,   containing OAuth flows for various providers (OSM, Google, etc).</li> </ul>"},{"location":"projects/future-plans/#building-tools-with-community-in-mind","title":"Building Tools With Community In Mind","text":"<p>The full end-to-end mapping flow is built with community mappers in mind. Empowering groups of people who wish to map the area they live in, and produce open data for the benefit of others.</p> <ul> <li>DroneTM: collective base imagery collection using affordable   drones distributed amongst a community of operators.</li> <li>OpenAerialMap: a centralized repository for aerial imagery, collected   from DroneTM or other sources within the aerial imagery community.</li> <li>fAIr: a local geo-AI marketplace for the community to contribute   feature prediction models, collectively test and rank model effectiveness,   and assist the digitization effort with AI.</li> <li>Tasking Manager: collaborative mapping amongst remotely distributed   communities.</li> <li>FieldTM: collaborative field mapping amongst local   mapping groups (e.g. regional OSM chapters, or local NGOs).</li> <li> <p>Export Tool: simple data export from OpenStreetMap, as a free service   to the OSM community and mappers globally.</p> <p>Each of these tools needs continuous improvement. User roadmaps are available on the respective github pages</p> </li> </ul>"},{"location":"projects/future-plans/#growing-our-software-dev-community","title":"Growing Our Software Dev Community","text":"<ul> <li>We want to continue to foster the growth of a software development   community around our tools.</li> <li>We should be open, inclusive, and welcome contribution in   many places.</li> <li>HOT has excellent connections to NGOs and organizations, plus a   very large community of active and engaged mappers, however,   we have limited software development capabilities due to the size   of our team!</li> <li>With some of our tools being recognized as   Digital Public Goods,   support the development of them has huge potential for impact on the   end users in our priority regions.</li> <li> <p>To facilitate this, we need to ensure that potential for contribution   is well publicized and common knowledge. The onboarding process should   be easy for new developers, plus we should be responsive and give back   to users who continually demonstate their willingness to contribute.</p> </li> </ul> <ul> <li>Every HOT tool should be hosted in a Kubernetes cluster.</li> <li>This means every tool needs to be containerized and follow   a 12factor app development approach.</li> <li>Tools should be deployed using a GitOps approach, via ArgoCD.   This means the container repositories for each tool are scanned,   and when a new version is uploaded, it is automatically deployed.</li> <li>Load is shared amongst tools on cluster nodes, reducing overall   costs of running multiple separate servers.</li> <li>Autoscaling capabilities when high load is detected for a tool,   automatically spawning new containers as needed.</li> </ul>"},{"location":"projects/future-plans/#cost-efficient-and-optimized-architecture","title":"Cost-efficient and optimized architecture","text":""},{"location":"projects/future-plans/#reduced-code-duplication","title":"Reduced Code Duplication","text":""},{"location":"projects/future-plans/#shared-ui-components","title":"Shared UI Components","text":"<p>See this page for more details.</p> <ul> <li>We don't have the resources to implement a very in-depth design   system across all of our tools.</li> <li>We also have existing tech debt due to development velocity and   no centralized design from the start (a need to move quickly...).</li> <li>The proposed solution is to converge around a single web component   library, such as Web Awesome.</li> <li>Web Awesome is free to use / open-source, has strong backers, and   would allow for a consistent style across all tools, regardless of   framework used (framework-agnostic, web standards).</li> <li>It should also improve maintainability, as part of the promise of web   components, is minimal requirement to upgrade into the future, as the   World Wide Web Consortium (W3C) does not break existing web functionality.</li> <li>We would also avoid the framework churn, by having our underlying   building blocks as web components, while building our wrapper apps in   whichever technology we choose.</li> <li>A few additional helper components will be designed and made available   as part of the HOTOSM UI library. For   example, a consistent header, footer, and sidebar for every tool.</li> </ul>"},{"location":"projects/future-plans/#shared-map-components","title":"Shared Map Components","text":"<p>See this page for more details.</p> <ul> <li>Historically we may have used OpenLayers and Leaflet in some places.   This should be consolidated around MapLibre.</li> <li>We have many implementations of MapLibre in our different tools. Ideally   we can use the same wrapper implementation, improving developer experience   and speed of development.</li> <li>Option 1: contribute to an existing established tool, that can compile   to plain JavaScript. This JS could be wrapped in a web-component for use in   any web app, any framework.   A top candidate would be   svelte-maplibre.</li> <li>Option 2: build a web component wrapper for MapLibre from scratch. It   is anticipated that community support would be high for this, being entirely   framework agnostic and embeddable anywhere.</li> </ul>"},{"location":"projects/future-plans/#python-modules-backend","title":"Python Modules (Backend)","text":"<p>See this page for more details.</p> <p>We have a lot of duplicated logic that could be centralised:</p> <ul> <li>Parsing of user uploaded GeoJSON AOIs (not as simple as it   sounds to do right).   geojson-aoi-parser</li> <li>Download of OSM data via raw-data-api. Every tool doing this should   use the raw-data-api-py   wrapper.</li> <li>A bunch of OpenStreetMap-specific XLSForm surveys have been started   in osm-fieldwork, but this   work needs some good community testing and peer review. The idea is to   plug the XLSForms into FieldTM, for easy OSM-centric mapping of an area   (to improve the map tags available, then push to OSM).</li> </ul>"},{"location":"projects/future-plans/#other-ideas","title":"Other Ideas","text":""},{"location":"projects/future-plans/#ai-llms","title":"AI / LLMs","text":"<ul> <li>OSM tag extraction from images or free text. Some work has started   on this in osm-tagger.</li> <li>A geospatial query chatbot, based on LLMs. Users want data for an   area. The LLM can provide commands / ways to get this data, either   via raw-data-api, overpass, or other available tools.</li> </ul>"},{"location":"projects/future-plans/#easy-osm-conflation","title":"Easy OSM Conflation","text":"<ul> <li>Various tools in our stack need to contribute data back to OSM, merging   it with the existing data present there (conflation).</li> <li>There are two possible approaches we could take, listed below.</li> </ul>"},{"location":"projects/future-plans/#conflation-web-component","title":"Conflation Web Component","text":"<ul> <li>A standalone 'conflation' web component that could be embedded in any   tool.</li> <li>It will load various data sources: OSM, custom data, Overture, etc.</li> <li>Various imagery sources can be loaded in. Ideally high-resolution drone   or recent open satellite imagery.</li> <li>The UI allows the user to select the best geometry / tags for a given   feature, based on the available imagery.</li> <li>The feature is validated by others (optional), then uploaded to OSM   as the new source of truth.</li> <li>If features came from FieldTM are field verified they should   take precedence, and will be tagged as such in OSM.</li> </ul>"},{"location":"projects/future-plans/#tasking-manager-conflation","title":"Tasking Manager Conflation","text":"<ul> <li>Use Tasking Manager as a collaborative conflation tool.</li> <li>Create a new 'conflation' project type.</li> <li>Users load the imagery, and datasets required.</li> <li>Tasking allows the conflation process to be subdivided amongst   the community, and validated.</li> <li>Final data is uploaded to OSM.</li> </ul>"},{"location":"projects/future-plans/#embeddable-web-components","title":"Embeddable Web Components","text":"<p>Three projects spring to mind, where it would be really neat to have a small lightweight web component, that could be embedded into any web tool / web page:</p> <ul> <li>GCP Editor: developed for DroneTM,   as a way to mark Ground Control Points on to captured images for more   accurate georeferencing of the final imagery mosaic.</li> <li>FieldTM Splitter: this   has yet to be developed, but the idea would be to have a self-contained   splitting component, where an AOI can be divided based on certain criteria   into task area. All of our tasking-manager type tools could benefit from this,   in addition to organization wishing to subdivide areas for field activities.</li> <li>OSM-Tagger: development of a web   component is underway that would receive images or text from a user, and extract   out potential OSM tags that could be applicable to the final dataset.</li> </ul> <p>In general, it's great to have a completely standalone web component, with no requirement to call an API.</p> <p>This could be achievable by using JavaScript libs, WebAssembly, and awesome projects such as PGLite, an embedded Postgres database in the browser.</p> <p>For example, OSM-Tagger will probably always rely on an external API due to it's reliance on Python Machine Learning libraries (this may change in future as JavaScript / Web Machine Learning libraries improve).</p> <p>But FieldTM Splitter could utilise PGLite to do the SQL splitting algorithms entirely in the web browser.</p>"},{"location":"projects/future-plans/#qgis-plugins-easy-data-access","title":"QGIS Plugins: Easy Data Access","text":"<ul> <li>Now we have raw-data-api-py,   it would be quite simple to make a QGIS plugin for this. Users of QGIS   could download the latest OSM data directly into their project.</li> <li>We could also have a QGIS plugin to easily access data via the   Humanitarian Data Exchange</li> </ul> <p>Note</p> <p>Before embarking on these plugin ideas, we should first assess if there is an actual need amongst communities first. Nothing worse than developing something that isn't actually needed / wanted.</p>"},{"location":"projects/future-plans/#xlsform-builder-repository","title":"XLSForm Builder &amp; Repository","text":"<p>Two problems:</p> <ol> <li>XLSForms can be a bit tricky to build directly. Via a drag-and-drop    web-ui would be easier.</li> <li>Lots of duplication of XLSForms across projects. People would benefit    from a centralised place to access some existing XLSForms.</li> </ol> <p>Idea:</p> <ul> <li>We create a basic website that has an embedded form builder.</li> <li>The website also acts as an XLSForm resository, where any user   can upload and contribute the XLSForm they designed, along with   a description for what they were mapping.</li> <li>The form builder part should be a web component, so it can also   be embedded into other sites too.</li> </ul>"},{"location":"projects/openaerialmap/","title":"Other Tools","text":""},{"location":"projects/openaerialmap/#openaerialmap-oam","title":"OpenAerialMap (OAM)","text":"<p>Important</p> <p>OpenAerialMap was originally developed by HOT in 2015 and has since undergone multiple revisions.</p> <p>HOT and partners are now actively seeking for funding to maintain and upgrade OAM as it is a critical tool in the Open Mapping System.</p> <p>Links:</p> <ul> <li>Imagery browser</li> <li>API</li> </ul>"},{"location":"projects/tools-summary/","title":"HOTOSM Tools","text":""},{"location":"projects/tools-summary/#data-types","title":"Data Types","text":"<p>There are two main types of geospatial data we must consider for maps.</p>"},{"location":"projects/tools-summary/#raster","title":"Raster","text":"<p>This is imagery data - a picture (jpeg, png, tiff, etc).</p> <p>On the lowest level the data is represented by pixels.</p> <p>The pixels are assigned a value within a band:</p> <ul> <li>Single band: the image could be greyscale, where the pixel value   contains a level of gray on a spectrum from white-black.</li> <li>Multi band: most images encountered in the OSM world will contain three   bands (RGB), which can be imagined to be stacked on top of one another.   Red, green, and blue are primary colours than when combined can form   any other colour.</li> </ul> <p>We are mostly interested in multi-band optical imagery, i.e. what you may see on Google / Bing Satellite.</p> <p>This underpins the usage of other data, by visually locating it in space - it tells you where your data is!</p>"},{"location":"projects/tools-summary/#vector","title":"Vector","text":"<p>This is point, polygon, and line data - what you find on OSM!</p> <p>The shapes are represented by some complex maths underneath and rendered on your screen.</p> <p>This is the data that is typically layered on top of basemaps (either raster or vector-based basemaps) for usage.</p>"},{"location":"projects/tools-summary/#how-is-this-data-used","title":"How is this data used?","text":""},{"location":"projects/tools-summary/#input-data","title":"Input Data","text":"<ul> <li>Raster Basemaps:<ul> <li>Google, Bing, ESRI, Mapbox all provide web basemaps we can use.</li> <li>Open Aerial Map (OAM) provides more bespoke basemaps of particular areas   of interest (AOIs), over a certain time period. Often higher resolution.</li> </ul> </li> <li>Vector Data:<ul> <li>OpenStreeMap (OSM) is the biggest reference of open map data.<ul> <li>HOT's Export Tool &amp; Raw Data API provide easy access to this.</li> </ul> </li> <li>Any other vector data in various formats:<ul> <li>GIS layers provided by governments: region boundaries, roads, etc.</li> <li>Data collected by NGOs and other organizations, open or not.</li> </ul> </li> </ul> </li> </ul> <p>All of this data can be used for various purposes, involving processing and data analysis.</p>"},{"location":"projects/tools-summary/#processed-data","title":"Processed Data","text":"<p>Generally always vector data in our context.</p> <p>It's OSM vector data that has been processed and packaged in a certain way so it can be used by other tools / users.</p> <p>Using the input data described above, we want to produce data with additional value:</p> <ul> <li>To generate new data to feed back into OSM (a complete loop).</li> <li>For other purposes such as humanitarian response, data analysis   and reporting.</li> </ul>"},{"location":"projects/tools-summary/#hots-tools","title":"HOT's Tools","text":"<p>With the above as context, HOT's tools roughly can be categorised as such:</p>"},{"location":"projects/tools-summary/#tool-division","title":"Tool Division","text":""},{"location":"projects/tools-summary/#input","title":"Input","text":"Tool Description Drone TM Get base imagery. OAM Stored and accessible base imagery. Raw Data API Extract data from OSM easily for software. <p>Note: input may be into our own tools, or workflows of others.</p>"},{"location":"projects/tools-summary/#output","title":"Output","text":"Tool Description TM Digitize map features remotely. fAIr Speed up the remote digitization process FMTM Add extra information to digitised features in the field. Export Tool Extract data from OSM easily for humans."},{"location":"projects/tools-summary/#project-summaries","title":"Project Summaries","text":""},{"location":"projects/tools-summary/#drone-tm","title":"Drone TM","text":"<p>Drone TM is the newest tool being developed by HOT.</p> <p>It's purpose is to fill the gap of reliable, high-quality, base imagery that can be used during the digitization process.</p> <p>Given an AOI, an area can be subdivided into task areas and flight plans generated for each users specific drone.</p> <p>In a collaborative manner a large amount of high quality base imagery can be generated very quickly and with minimal cost.</p>"},{"location":"projects/tools-summary/#openaerialmap-oam","title":"OpenAerialMap (OAM)","text":"<p>OAM should underpin all of our tools.</p> <p>It is essentially a repository of base imagery, which is ideally high-resolution for a specifically targetted mapping area.</p> <p>Imagery can be uploaded from Drone TM, or any other openly available data source, by the community.</p> <p>The imagery is made available to other tools via the Tile Map Service protocol (TMS).</p>"},{"location":"projects/tools-summary/#raw-data-api","title":"Raw Data API","text":"<p>Uses an innovative database structure to make OSM data much more easily searchable with excellent performance.</p> <p>This database is made accessible behind an API, where users (or software) can request OSM data within specific filter criteria.</p> <p>Data can be exported in various formats, from GeoJSON to GeoPackage and Flatgeobuf.</p>"},{"location":"projects/tools-summary/#export-tool","title":"Export Tool","text":"<p>The purpose of Export Tool is to:</p> <ul> <li>Take the users input for what data they need, over what area.</li> <li>Calls Raw Data API to extract and filter the data.</li> <li>Receives back the data in the user requested format.</li> </ul> <p>This is essentially the more user-centric frontend for raw-data-api.</p>"},{"location":"projects/tools-summary/#tasking-manager","title":"Tasking Manager","text":"<p>Creating additional vector data to go into OSM, using raster imagery as a source.</p> <p>Pretty much creating polygons and lines from things you and see on a map and uploading them to OSM.</p> <p>The mapping is crowdsourced, or organised by NGOs.</p> <p>Resulting data needs to be validated.</p> <p>Another tool, MapSwipe, also fills this niche but takes a much more minimalistic approach.</p> <p>It allows users without any prior OSM or mapping experience to contribute more effectively, from a mobile device or web browser.</p>"},{"location":"projects/tools-summary/#fair","title":"fAIr","text":"<p>TM is labour intensive. fAIr uses localised training data to generate a model that can be used to predict vector features from a raster image.</p> <p>Integration into TM means mapping of features can be a lot faster.</p>"},{"location":"projects/tools-summary/#fmtm","title":"FMTM","text":"<p>Adding useful tags, in the field, from the vector data created in TM.</p> <p>The tags provide extra information about what features actually are in OSM.</p> <p>Again, the mapping can be crowdsourced, but more likely organised by NGOs / govs.</p> <p>The FMTM mapping could also provide a feedback loop to TM, helping to validate that features were mapped correctly.</p> <p>As with TM, another tool, StreeComplete, can help with this &amp; may be part of the solution.</p> <p>However, FMTM allows mapping to be done collaboratively.</p>"},{"location":"techdoc/","title":"HOT Documentation","text":"<p>Project-specific documentation can be found in the docs folder for the project. This is the home of everything that does not belong to one specific project.</p> <p>Here you can find:</p>"},{"location":"techdoc/#learn-about-the-hot-documentation-approach","title":"Learn about the HOT Documentation Approach","text":""},{"location":"techdoc/#technical-documentation-templates","title":"Technical Documentation Templates","text":"<p>Standard documentation templates for HOT technical documentation.</p>"},{"location":"techdoc/#learn-about-the-hot-architecture","title":"Learn About the HOT Architecture","text":""},{"location":"techdoc/#overarching-views-of-the-architecture","title":"Overarching Views of the Architecture","text":"<p>Architecture views (and other documentation) that is not specific to one HOTSM application and applicable across multiple or all.</p>"},{"location":"techdoc/#the-architecture-decision-log","title":"The Architecture Decision Log","text":"<p>A register of all the key architecture decisions for the HOTSM ecosystem, including some detail around why each was decided.</p>"},{"location":"techdoc/#risk-log","title":"Risk Log","text":"<p>A register of all the overarching risks for the HOTSM ecosystem.</p> <p>You can visit the Humanitarian OpenStreetMap Team at https://github.com/hotosm and check out our pinned projects or browse through the full family of HOTSM projects.</p>"},{"location":"techdoc/#structures-to-improve-github-documentation","title":"Structures To Improve GitHub Documentation","text":"<p>An overall documentation structure outline, that cuts across; Project, Product, System and Process documentation. These structures can be replicated for all forms of documentation needed by HOTOSM. It also includes specific tips for improving already existing documentation.</p>"},{"location":"techdoc/#goals","title":"Goals","text":"<ol> <li>Highlight the definitions of each type of documentation that are    commonly used on Github.</li> <li>Provide a foundation to lay information on top of, in order to    produce efficient documentation about any product or project in the    future.</li> <li>List key parts of each documentation type that aid the flow and    experience of the reader.</li> <li>Outline general tips to improve already existing documentation.</li> </ol>"},{"location":"techdoc/#common-documentation-types-on-github","title":"Common Documentation Types On Github","text":"<p>I have found 4 main documentation types through my own personal research. They can all be written individually but often coincide to complement specific parts of each other. For example, Product documentation may include some Process documentation under the Usage section, demonstrating how to use that product. These 4 types in full, are as follows:</p> <ul> <li>Product Documentation</li> <li>Project Documentation</li> <li>System Documentation</li> <li>Process Documentation</li> </ul>"},{"location":"techdoc/#1-structure-for-product-documentation","title":"1. Structure for Product Documentation","text":"<p>Product documentation is the process of recording key information (almost everything you need to know) about a product, including how to use it. Product documentation may have form of Process documentation within it (this will be further explained later on). A flexible and reusable structure of essential components of product documentation is as follows:</p> <ul> <li>Overview</li> <li>Introduction: What the product is, what it does, the target audience, etc.</li> <li>Features: A breakdown of each integral part of the product, their   functionality and purpose</li> <li>Benefits: How the features give users an edge over other similar   products. In other words, what's in it for the user.</li> <li>Usage: A step by step process of how to use the product</li> <li>Support / Frequently Asked Questions (about the product)</li> <li>License</li> </ul> <p>The points stated above can be used in that order and can also be switched around based on convenience and the type of product. Additionally, other points can be added if necessary, but these are what I believe to be some of the most important.</p>"},{"location":"techdoc/#2-structure-for-project-documentation","title":"2. Structure for Project Documentation","text":"<p>Project documentation is the process of recording the key project details that are needed to implement a project. It's like a roadmap of what the project is and all necessary information about what it entails. Main structural components are in the following order:</p> <ul> <li>Overview</li> <li>Vision</li> <li>Aim / Mission</li> <li>Introduction</li> <li>Project plan</li> <li>Project schedule</li> <li>Tools and Features</li> <li>Access</li> <li>Resources</li> <li>Support / Guidelines</li> </ul>"},{"location":"techdoc/#3-structure-for-system-documentation","title":"3. Structure for System Documentation","text":"<p>System documentation is an all-encompassing record of details of a full working system. It is very similar to the structure of product documentation but it's usually on a wider scale. It may even include some forms of product and process documentation within it. In addition to the structure of product documentation above, other key components it might include are: architecture design, program source code and maintenance / help guide.</p>"},{"location":"techdoc/#4-structure-for-process-documentation","title":"4. Structure for Process Documentation","text":"<p>Process documentation is capturing and listing all steps involved in doing a task. It is the full explanation of a process, step by step. Process documentation is very useful in other documentation forms. As mentioned earlier, it can be used within System, Product or Project documentation, to explain a process. The format is usually:</p> <ul> <li>Overview</li> <li>Introduction</li> <li>Explanation steps (breakdown of the task)</li> <li>Help (if any problems occur while acting on the steps)</li> </ul> <p>The structures I have suggested above are similar, consistent and can be easily replicated for various projects, systems and products. They work well when integrated together, balancing out issues where necessary. Each structure also serves as a foundation that can be added to and built upon.</p>"},{"location":"techdoc/#improving-existing-documentation","title":"Improving Existing Documentation","text":"<ol> <li> <p>Acronyms should be stated in full before repetitive    use. Acronyms like ODK, OSM, etc, should be stated in full before    use or after each use within brackets e.g ODK (ODK). This    negates confusion for readers / users and just simplifies things.</p> </li> <li> <p>Important features should be put in bold. For example \u201cselect    from map\u201d and 'ODK Collect' can be easily overlooked by readers if    they aren't highlighted, even though they are important features.</p> </li> <li> <p>Generally simplifying words and phrases. This makes    documentation more user friendly and much easier to understand,    since users come from all different backgrounds. For example:</p> <p>\u201cODK incorporates a new functionality\u201d can become \u201cODK has brought in a new feature\u201d.</p> <p>\u201cField Mappers select (or are allocated) individual tasks within a  project's AOI\u201d could be changed to \u201cField Mappers choose or are  given tasks that are part of a project's Areas Of Interest.\u201d</p> </li> <li> <p>Avoid long paragraphs. Short paragraphs that pass a clear    message are less clumsy and flustering for readers. Breaking down    topics into little, easy to understand chunks, is more user    friendly.</p> </li> <li> <p>Maintain a positive tone in the writing.. Keep the text    positive and informative. Avoid words like 'obviously' and    'basically', that may be interpreted as demeaning or    condescending. Do not expect readers to have a certain amount of    knowledge on specific aspects, break down everything that needs to    be broken down.</p> </li> <li> <p>Constantly update your documentation. Projects, products and    systems are always evolving. Thus, it is necessary to keep    documentation up to date with any new changes.</p> </li> <li> <p>Consistency in the documentation format. Maintaining    consistency is a key aspect of creating clear and effective    documentation.This consistency includes but is not limited to    elements like numbering, font styles, heading sizes, and spacing.</p> <p>Using the same font for all headings and subheadings can help  readers quickly identify important sections of the  document. Similarly, using consistent spacing between paragraphs  and sections can make the document more visually appealing and  easier to follow. This helps to create a documentation that is  clear, effective, and easy to use.</p> </li> </ol>"},{"location":"techdoc/overview/","title":"Overviews","text":"<p>While today HOT architecture consists of a wide range of applications and modules, the vision for the future is a more unified ecosystem.</p> <p>The key features of this architecture are:</p> <ul> <li>An integrated set of back-end modules</li> <li>A common data model based on standards and only extending as   required</li> <li>Fit for purpose front-ends, built on REACT</li> </ul>"},{"location":"techdoc/overview/#hot-overview","title":"HOT Overview","text":"<p>This is an overarching view of the HOT ecosystem of solutions. It includes some key non-HOT components for context and because they serve an important role in the ecosystem.</p> <p></p>"},{"location":"techdoc/overview/#solution-user-view","title":"Solution User View","text":"<p>This solution user view provides a user-centric view of the HOT architecture showing the key user roles and what activities they will perform using the solution. You can learn more about the diagram and notation here.</p> <p></p>"},{"location":"techdoc/overview/#information-flow","title":"Information Flow","text":"<p>This information flow is a dynamic view that shows the flow of information (in high level business terms) between the HOT ecosystem components. You can learn more about the diagram and notation here.</p> <p></p>"},{"location":"techdoc/overview/#component-model","title":"Component Model","text":"<p>This component model is a static view that shows how the HOT ecosystem components connect to one another. You can learn more about the diagram and notation here.</p> <p></p>"},{"location":"techdoc/overview/#end-to-end-e2e","title":"End-To-End (E2E)","text":"<p>This is a plan, currently in progress, to have a better end user experience between multiple HOT projects. Since often multiple projects are used, Tasking Manager, Export Tool, Field Mapping Tasking Manager, etc... the journey between them should be efficient.</p>"},{"location":"techdoc/overview/#e2e-data-integration-diagrams","title":"E2E Data Integration Diagrams","text":"<p>This is the high level concept for the E2E data integration approach. The TD Admin components in the center are not actually a hub instance, but a set of shared modules used for connectivity.  </p>"},{"location":"techdoc/overview/#e2e-integration-sequence-diagrams","title":"E2E Integration Sequence Diagrams","text":"<p>These show the interactions between multiple components at a more detailed level.</p> <p>|  |  | |  | . |</p>"},{"location":"techdoc/overview/#e2e-conceptual-data-model","title":"E2E Conceptual Data Model","text":"<p>This is a conceptual data model, illustrating the key entities and relationship in the data model. </p>"},{"location":"techdoc/templates/","title":"HOT Documentation Templates","text":"<p>These are templates that can be used to document HOT's tech projects.</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"techdoc/templates/Decisions/","title":"Decisions","text":"<p>All italics is helper text. Delete when filling out. More guidance on decisions here: https://wittij.com/solution-architecture-decisions</p>"},{"location":"techdoc/templates/Decisions/#state-the-decision-here","title":"State the Decision Here","text":"<p>The decision should be stated as the top level heading; as an assertion: \"Front ends will be built using REACT.\"</p>"},{"location":"techdoc/templates/Decisions/#analysis-options-considered","title":"Analysis / Options Considered","text":"<p>Summary of analysis leading to the decision. Should include options considered, pros and cons to each, and explanation of why this was decided. Try to minimize long narrative - prefer use of structured text (bullets and tables) and illustrations instead. Can also link to an external options analysis. See more on options analysis here: https://wittij.com/solution-options-analysis</p>"},{"location":"techdoc/templates/Decisions/#related-information","title":"Related Information","text":"<p>Links to external information that is of use to this decision</p>"},{"location":"techdoc/templates/Decisions/#bad-decision","title":"Bad Decision","text":"<p>This section is only completed if the decision is know to not be the right long term one.</p>"},{"location":"techdoc/templates/Decisions/#ideal-target","title":"Ideal Target","text":"<p>What the decision should have been if this is not it!</p>"},{"location":"techdoc/templates/Decisions/#factors","title":"Factors","text":"<p>Most typical factors are not enough funding, not enough time, or lack of capabilities from a component of the architecture</p>"},{"location":"techdoc/templates/Risks/","title":"Risk is stated Here","text":""},{"location":"techdoc/templates/Risks/#impact","title":"Impact","text":"<p>Narrative explaining what bad will happen if this risk happens.</p>"},{"location":"techdoc/templates/Risks/#severity","title":"Severity","text":"<p>Hoe bad will it be? Severity of the risk on this scale: negligible, marginal, critical, catastrophic.</p>"},{"location":"techdoc/templates/Risks/#likelihood","title":"Likelihood","text":"<p>How possible is it that this happens? Likelihood of the risk on this scale: rare, unlikely, possible, likely, certain.</p>"},{"location":"techdoc/templates/Risks/#response","title":"Response","text":"<p>Narrative explanation of how this risk will be addressed. Will be a combination of how the risk will be managed. Some common approaches are: mitigation, monitoring, prepared a response, contracting to another party.</p>"}]}